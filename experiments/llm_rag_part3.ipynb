{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Retrival logic discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import polars as pl\n",
    "import pymupdf\n",
    "import torch\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PDF to be parsed, chunked and embedded\n",
    "pdf_path = BASE_DIR / \"data\" / \"human_nutrition_text.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def text_formatter(text: str) -> str:\n",
    "\t\"\"\"Performs minor text formatting.\"\"\"\n",
    "\timport re\n",
    "\n",
    "\tcleaned_text = re.sub(\n",
    "\t\tr\"\\s+\", \" \", text\n",
    "\t)  # Replace multiple whitespace with single space\n",
    "\tcleaned_text = cleaned_text.strip()\n",
    "\treturn cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf(file_path: Union[str, Path]) -> Union[List[Dict], None]:\n",
    "\t\"\"\"\n",
    "\tOpens a pdf file and reads its content page by page, and collects statistics.\n",
    "\tParameters:\n",
    "\t    file_path (str | Path): The path to the pdf file to be opened and read.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries containing the page number, character count, word count, sentence count, token count, and extracted text for each page.\n",
    "\t\"\"\"\n",
    "\tif not Path(file_path).exists():\n",
    "\t\traise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "\ttry:\n",
    "\t\tdoc = pymupdf.open(file_path)\n",
    "\t\tpages_and_texts = []\n",
    "\t\tfor page_number, page in tqdm(enumerate(doc)):\n",
    "\t\t\ttext = page.get_text()\n",
    "\t\t\tif not text or not text.strip():  # Skip empty pages\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif text and text.strip():\n",
    "\t\t\t\ttext = text_formatter(text)\n",
    "\t\t\t\tsentences = re.split(r\"[.!?]+\", text)  # Simple sentence splitter\n",
    "\t\t\t\tsentence_count = len(\n",
    "\t\t\t\t\t[s for s in sentences if s.strip()]\n",
    "\t\t\t\t)  # Count non-empty sentences\n",
    "\t\t\t\tpages_and_texts.append(\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"page_number\": page_number - 41,\n",
    "\t\t\t\t\t\t\"page_char_count\": len(text),\n",
    "\t\t\t\t\t\t\"page_word_count\": len(text.split()),\n",
    "\t\t\t\t\t\t\"page_sentence_count_raw\": sentence_count,\n",
    "\t\t\t\t\t\t\"page_token_count\": int(len(text) / 4),\n",
    "\t\t\t\t\t\t\"text\": text,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\treturn pages_and_texts\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error reading PDF file: {e}\")\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts = open_and_read_pdf(file_path=pdf_path)\n",
    "if pages_and_texts:\n",
    "\tprint(pages_and_texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on all pages\n",
    "for item in tqdm(pages_and_texts):\n",
    "\titem[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "\t# make sure all sentences are strings\n",
    "\titem[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "\t# Count the sentences\n",
    "\titem[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now splitting sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "\n",
    "def split_sentence_list(input_list: list[str], slice_size: int) -> list[list[str]]:\n",
    "\t\"\"\"Function takes list of sentences as input and slices it based in slice size\n",
    "\tArgs:\n",
    "\t    input_list (list[str]): list of sentences\n",
    "\t    slice_size (int): a number to slice the input list by\n",
    "\tReturns:\n",
    "\t    list[list[str]]: two list created based on list slice functionality\n",
    "\tExample:\n",
    "\t    An input list of 17 sentences with 10 as slice size will return two list:\n",
    "\t    1) list of first 10 sentences, 2) list of remaining 7 sentences\n",
    "\t\"\"\"\n",
    "\treturn [\n",
    "\t\tinput_list[i : i + slice_size] for i in range(0, len(input_list), slice_size)\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chunks to pages_and_texts\n",
    "for item in tqdm(pages_and_texts):\n",
    "\titem[\"sentence_chunks\"] = split_sentence_list(\n",
    "\t\tinput_list=item[\"sentences\"], slice_size=num_sentence_chunk_size\n",
    "\t)\n",
    "\titem[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting chunks into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "\tfor sentence_chunk in item[\"sentence_chunks\"]:\n",
    "\t\tchunk_dict = {}\n",
    "\t\tchunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "\t\t# join the sentence together to make a paragraph like structure.\n",
    "\t\tjoined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "\t\tjoined_sentence_chunk = re.sub(\n",
    "\t\t\tr\"\\.([A-Z])\", r\". \\1\", joined_sentence_chunk\n",
    "\t\t)  # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
    "\t\tchunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "\t\t# Stats\n",
    "\t\tchunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "\t\tchunk_dict[\"chunk_word_count\"] = sum(\n",
    "\t\t\t1 for word in joined_sentence_chunk.split()\n",
    "\t\t)\n",
    "\t\tchunk_dict[\"chunk_token_count\"] = round(len(joined_sentence_chunk) / 4, 2)\n",
    "\t\tpages_and_chunks.append(chunk_dict)\n",
    "print(f\"We have {len(pages_and_chunks)} chunks now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data structure to dataframe\n",
    "df = pl.DataFrame(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks which are less 30 in length provide little information, so removing them from the dataframe\n",
    "min_token_length = 30\n",
    "pages_and_chunks_over_min_token_len = df.filter(\n",
    "\tpl.col(\"chunk_token_count\") > min_token_length\n",
    ").to_dicts()\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and setup embedding model\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to tensor as required by SentenceTransformer dot env functionality\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "\tembedding_array = model.encode_document(\n",
    "\t\titem[\"sentence_chunk\"], convert_to_tensor=True\n",
    "\t)\n",
    "\titem[\"embedding\"] = embedding_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### let's write code for code similarity based on dot product, then sort the result based on similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"micronutrients functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = df[\"embedding\"].to_list()\n",
    "embeddings = torch.tensor(embeddings)\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "print(\n",
    "\tf\"Time to get scores on {len(df['embedding'])} embeddings: {end_time - start_time} seconds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for printing\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "\twrapped_text = textwrap.fill(text, wrap_length)\n",
    "\tprint(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {query}\")\n",
    "print(\"Results:\")\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "\tprint(f\"Score: {score:.4f}\")\n",
    "\tprint(\"Text: \\n\")\n",
    "\tprint_wrapped(df[\"sentence_chunk\"][int(idx)])\n",
    "\tprint(f\"Page Number: {df['page_number'][int(idx)]}\")\n",
    "\tprint(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating final semantic search pipeline\n",
    "def retrieve_relevant_resources(\n",
    "\tquery: str,\n",
    "\tembeddings: list,\n",
    "\tmodel: SentenceTransformer,\n",
    "\tn_resources_to_return: int,\n",
    "\tprint_time: bool = True,\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "\t\"\"\"Embeds the query and retrieves the top n_resources_to_return most relevant resources's score and index.\n",
    "\tArgs:\n",
    "\t    query (str): The query to search for.\n",
    "\t    embeddings (list): List of embeddings to search for the query in.\n",
    "\t    model (SentenceTransformer): The SentenceTransformer model to use for embedding.\n",
    "\t    n_resources_to_return (int): The number of resources to return.\n",
    "\t    print_time (bool): print the time taken to retrieve the resources.\n",
    "\tReturns:\n",
    "\t    None\n",
    "\t\"\"\"\n",
    "\tstart_time = timer()\n",
    "\tquery_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\tdot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "\tscores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "\tend_time = timer()\n",
    "\tif print_time:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Time to get scores on {len(embeddings)} embeddings: {end_time - start_time} seconds.\"\n",
    "\t\t)\n",
    "\treturn scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_results_and_scores(\n",
    "\tquery: str,\n",
    "\tembeddings: list,\n",
    "\tpages_and_chunks: list[dict] = df,\n",
    "\tn_reources_to_return: int = 5,\n",
    "):\n",
    "\t\"\"\"This function prints the top n_reources_to_return most relevant resources's score and index.\"\"\"\n",
    "\tscores, indices = retrieve_relevant_resources(\n",
    "\t\tquery=query,\n",
    "\t\tembeddings=embeddings,\n",
    "\t\tmodel=model,\n",
    "\t\tn_resources_to_return=n_reources_to_return,\n",
    "\t)\n",
    "\tprint(f\"Query: {query}\")\n",
    "\tprint(\"Results: \\n\")\n",
    "\tfor score, idx in zip(scores, indices):\n",
    "\t\tprint(f\"Score: {score:.4f}\")\n",
    "\t\tprint(\"Text: \\n\")\n",
    "\t\tprint_wrapped(pages_and_chunks[\"sentence_chunk\"][int(idx)])\n",
    "\t\tprint(f\"Page Number: {pages_and_chunks['page_number'][int(idx)]}\")\n",
    "\t\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"symptoms of pellagra\"\n",
    "scores, indices = retrieve_relevant_resources(\n",
    "\tquery=query, embeddings=embeddings, model=model, n_resources_to_return=5\n",
    ")\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_results_and_scores(query=query, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
