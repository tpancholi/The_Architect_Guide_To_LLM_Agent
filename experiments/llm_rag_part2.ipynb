{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Embedding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from experiments.rag_chunking_strategy_part2 import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Project root set to: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = BASE_DIR / \"data\" / \"human_nutrition_text.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "\t\"\"\"Performs minor text formatting.\"\"\"\n",
    "\timport re\n",
    "\n",
    "\tcleaned_text = re.sub(\n",
    "\t\tr\"\\s+\", \" \", text\n",
    "\t)  # Replace multiple whitespace with single space\n",
    "\tcleaned_text = cleaned_text.strip()\n",
    "\treturn cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf(file_path: Union[str, Path]) -> Union[List[Dict], None]:\n",
    "\t\"\"\"\n",
    "\tOpens a pdf file and reads its content page by page, and collects statistics.\n",
    "\tParameters:\n",
    "\t    file_path (str | Path): The path to the pdf file to be opened and read.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries containing the page number, character count, word count, sentence count, token count, and extracted text for each page.\n",
    "\t\"\"\"\n",
    "\tif not Path(file_path).exists():\n",
    "\t\traise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "\ttry:\n",
    "\t\tdoc = pymupdf.open(file_path)\n",
    "\t\tpages_and_texts = []\n",
    "\t\tfor page_number, page in tqdm(enumerate(doc)):\n",
    "\t\t\ttext = page.get_text()\n",
    "\t\t\tif not text or not text.strip():  # Skip empty pages\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif text and text.strip():\n",
    "\t\t\t\ttext = text_formatter(text)\n",
    "\t\t\t\tsentences = re.split(r\"[.!?]+\", text)  # Simple sentence splitter\n",
    "\t\t\t\tsentence_count = len(\n",
    "\t\t\t\t\t[s for s in sentences if s.strip()]\n",
    "\t\t\t\t)  # Count non-empty sentences\n",
    "\t\t\t\tpages_and_texts.append(\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"page_number\": page_number - 41,\n",
    "\t\t\t\t\t\t\"page_char_count\": len(text),\n",
    "\t\t\t\t\t\t\"page_word_count\": len(text.split()),\n",
    "\t\t\t\t\t\t\"page_sentence_count_raw\": sentence_count,\n",
    "\t\t\t\t\t\t\"page_token_count\": int(len(text) / 4),\n",
    "\t\t\t\t\t\t\"text\": text,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\treturn pages_and_texts\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error reading PDF file: {e}\")\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts = open_and_read_pdf(file_path=pdf_path)\n",
    "if pages_and_texts:\n",
    "\tprint(pages_and_texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame(pages_and_texts)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Chunking Preparation \n",
    "- first step is to add sentences from page as new key value pair to `pages_and_texts` data structure\n",
    "- divide the sentences into two chunks\n",
    "    - **chunk-1**: 10 sentences\n",
    "    - **chunk-2**: rest of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing spacy to identify sentences from paragraph\n",
    "test_para = \"\"\"\n",
    "Simple string splitting methods are unreliable for sentence segmentation. They often fail on common text elements like abbreviations. For example, 'Dr. Smith' would be incorrectly split. Decimal values such as 3.14 also cause problems. spaCy's sentencizer component solves these issues effectively. It uses a trained model to identify true sentence boundaries. This model correctly handles abbreviations and decimals. It also manages quoted speech and ellipses properly. This provides a robust foundation for further text analysis. Therefore, spaCy offers a significant advantage over basic methods.\"\"\"\n",
    "list_sentences = [sent.text.strip() for sent in nlp(test_para).sents]\n",
    "print(f\"# of sentences identified: {len(list_sentences)}\")\n",
    "for i, sent in enumerate(list_sentences, 1):\n",
    "\tprint(f\"{i}:- {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on all pages\n",
    "for item in tqdm(pages_and_texts):\n",
    "\titem[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "\t# make sure all sentences are strings\n",
    "\titem[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "\t# Count the sentences\n",
    "\titem[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(pages_and_texts)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "\n",
    "if pages_and_texts:\n",
    "\tpprint.pp(pages_and_texts[random.randint(0, 1179)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now splitting sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "\n",
    "def split_sentence_list(input_list: list[str], slice_size: int) -> list[list[str]]:\n",
    "\t\"\"\"Function takes list of sentences as input and slices it based in slice size\n",
    "\tArgs:\n",
    "\t    input_list (list[str]): list of sentences\n",
    "\t    slice_size (int): a number to slice the input list by\n",
    "\tReturns:\n",
    "\t    list[list[str]]: two list created based on list slice functionality\n",
    "\tExample:\n",
    "\t    An input list of 17 sentences with 10 as slice size will return two list:\n",
    "\t    1) list of first 10 sentences, 2) list of remaining 7 sentences\n",
    "\t\"\"\"\n",
    "\treturn [\n",
    "\t\tinput_list[i : i + slice_size] for i in range(0, len(input_list), slice_size)\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chunks to pages_and_texts\n",
    "for item in tqdm(pages_and_texts):\n",
    "\titem[\"sentence_chunks\"] = split_sentence_list(\n",
    "\t\tinput_list=item[\"sentences\"], slice_size=num_sentence_chunk_size\n",
    "\t)\n",
    "\titem[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "\n",
    "if pages_and_texts:\n",
    "\tpprint.pp(pages_and_texts[random.randint(0, 1179)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(pages_and_texts)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## shifting from pages and texts to pages and chunks\n",
    "- currently we have pages and text (group of sentences) with multiple chunks with page as the item parent\n",
    "- we would now shift to chunk as the parent item with all the information as is. (assumption is size should atleast double from 1179 to ~2358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "\tfor sentence_chunk in item[\"sentence_chunks\"]:\n",
    "\t\tchunk_dict = {}\n",
    "\t\tchunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "\t\t# join the sentence together to make a paragraph like structure.\n",
    "\t\tjoined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "\t\tjoined_sentence_chunk = re.sub(\n",
    "\t\t\tr\"\\.([A-Z])\", r\". \\1\", joined_sentence_chunk\n",
    "\t\t)  # \".A\" -> \". A\" for any full-stop/capital letter combod\n",
    "\t\tchunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "\t\t# Stats\n",
    "\t\tchunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "\t\tchunk_dict[\"chunk_word_count\"] = sum(\n",
    "\t\t\t1 for word in joined_sentence_chunk.split()\n",
    "\t\t)\n",
    "\t\tchunk_dict[\"chunk_token_count\"] = round(len(joined_sentence_chunk) / 4, 2)\n",
    "\t\tpages_and_chunks.append(chunk_dict)\n",
    "print(f\"We have {len(pages_and_chunks)} chunks now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(pages_and_chunks)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove chunks which are less than 30 make the data a bit cleaner\n",
    "# the chunks which are less than 30 are usually related to footnotes, which can be seen by below code\n",
    "min_token_length = 30\n",
    "sample_df = df.filter(pl.col(\"chunk_token_count\") <= min_token_length).sample(10)\n",
    "for row in sample_df.iter_rows(named=True):\n",
    "\tprint(\n",
    "\t\tf\"Chunk Token Count: {row['chunk_token_count']} - Chunk: {row['sentence_chunk']}\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as seen above the chunk which are less 30 in length provide little information so removing them from dataframe\n",
    "pages_and_chunks_over_min_token_len = df.filter(\n",
    "\tpl.col(\"chunk_token_count\") > min_token_length\n",
    ").to_dicts()\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "documents = [\n",
    "\t\"Venus is often called Earth's twin because of its similar size and proximity.\",\n",
    "\t\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n",
    "\t\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n",
    "\t\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\",\n",
    "]\n",
    "document_embeddings = model.encode_document(documents)\n",
    "print(document_embeddings.shape)\n",
    "embedding_dict = dict(zip(documents, document_embeddings))\n",
    "\n",
    "# for document, embedding in embedding_dict.items():\n",
    "#     print(f\"Document: {document}\")\n",
    "#     print(f\"Embedding: {embedding}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "\titem[\"embedding\"] = model.encode_document(\n",
    "\t\titem[\"sentence_chunk\"], normalize_embeddings=True\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.to(\"cpu\")\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "\titem[\"embedding\"] = model.encode_document(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "\titem[\"embedding\"] = model.encode_document(\n",
    "\t\titem[\"sentence_chunk\"], normalize_embeddings=True\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "\ttext = text.replace(\"\\n\", \" \")\n",
    "\treturn client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "\titem[\"embedding\"] = get_embedding(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-8B\", token=os.getenv(\"HF_TOKEN\"))\n",
    "documents = [\n",
    "\t\"Venus is often called Earth's twin because of its similar size and proximity.\",\n",
    "\t\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n",
    "\t\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n",
    "\t\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\",\n",
    "]\n",
    "document_embeddings = model.encode_document(documents)\n",
    "print(document_embeddings.shape)\n",
    "embedding_dict = dict(zip(documents, document_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", token=os.getenv(\"HF_TOKEN\"))\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "\titem[\"embedding\"] = model.encode_document(\n",
    "\t\titem[\"sentence_chunk\"], normalize_embeddings=True\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_chunk_embeddings = model.encode(\n",
    "\ttext_chunks, batch_size=50, normalize_embeddings=True, show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_chunk_embeddings = model.encode_document(\n",
    "\ttext_chunks, batch_size=32, normalize_embeddings=True, show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizuara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
