{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Agentic RAG using LlamaIndex\n",
    "\n",
    "## What is LlamaIndex?\n",
    "- Initially it was developed specifically for a Retrival Augmented Generation use case.  Now it has evolved and is used for AI Agents as well.\n",
    "- Complete toolkit for context-augmented LLM applications.\n",
    "- Main Components of LlamaIndex:\n",
    "    - Data connectors (ingest and format existing data)\n",
    "    - Data Indexes (structure and store data to be consumed by LLM)\n",
    "    - Engines (Chat engine and Query engine)\n",
    "    - Agents (simple tools, helper function to API integrations)\n",
    "    - Observability / Evaluations\n",
    "    - Workflows (event-driven or graph-based system)\n",
    "- What makes it special?\n",
    "    - Easy document parsing using LlamaParse\n",
    "    - Many ready-to-use components\n",
    "    - Simple and clear workflow system\n",
    "    - LlamaHub (3rd party ready-to-use tools)\n",
    "\n",
    "\n",
    "## Why moving away from SmolAgent?\n",
    "- SmolAgent is a minimalistic library to create coding and tool calling agent.\n",
    "- Great for creating simple agents but becomes complicated for a complex or multiple task use case.\n",
    "- Single Agent uses long context and high token as well as prone to hallucination during complex reasoning tasks.\n",
    "- Multiple Agent overcomes the above issues, but the library lacks flexibility, # of out of box tools available and is not scalable.\n",
    "\n",
    "## What is RAG?\n",
    "- It is also known as grounded generation.\n",
    "- RAG is a technique extremely useful for creating chatbots.\n",
    "- It only provides relevant information to LLM to answer the user's query leading to better, faster, cheaper, and more relevant information.\n",
    "- Generic RAG Flow\n",
    "\t- User asks query -> LLM looks at vector database and retrieves relevant information -> LLM makes a decision based on the retrieved information. -> LLM sends the information to the user.\n",
    "\n",
    "## How is it different from Agentic-RAG?\n",
    "- sometime one pass might not be enough to answer user's query and need to go through multiple passes. (like ReACT pattern)\n",
    "- Traditional RAG has no access to external tools, might limit it's capabilities to get enough information to make a complex decision.\n",
    "- So we can conclude RAG system is an agent like memory, tools, reason, plan and external tools as well as a query engine as a tool.\n",
    "- Agentic RAG Flow\n",
    "\t- User Asks query -> LLM looks at does it have enough information to answer the query -> if not, it will look at different tools it has access and try to get the information -> LLM will look at the retrieved information and make a decision if enough information is available. -> if not, it will make a modification to the query and try again till it gets the information. -> then sends information to the user.\n",
    "\n",
    "## Why create RAG-based Agent\n",
    "- Reduction in hallucination\n",
    "- Better memory management\n",
    "- Updated knowledge base of llm\n",
    "\n",
    "## Using LLM-as-a-judge using LangFuse for any LLM application\n",
    "- Using a large LLM to review the responses generated by the agents and evaluate the quality of the responses.\n",
    "- LangFuse supports Ragas library Evolution metrics out-of-box.\n",
    "\n",
    "### Important Metrics\n",
    "- Hallucinations\n",
    "- Trustworthiness\n",
    "- Relevance\n",
    "- Correctness and completeness\n",
    "- Efficiency (token/time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Important libraries for simple and Agentic RAG from LlamaIndex\n",
    "- llama-index\n",
    "- llama-index-vector-stores-chroma\n",
    "- llama-index-embeddings-openai\n",
    "- llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Part 0: Setup foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 0.1 Setup environment and required paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your environment variables\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PHOENIX_API_KEY = os.getenv(\"PHOENIX_API_KEY\")\n",
    "BRAVE_SEARCH_API_KEY = os.getenv(\"BRAVE_SEARCH_API_KEY\")\n",
    "print(\"Environment variables set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup path\n",
    "from pathlib import Path\n",
    "\n",
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent\n",
    "pdf_path = BASE_DIR / \"data\" / \"the-state-of-ai.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 0.2 Document preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "reader = SimpleDirectoryReader(input_files=[pdf_path])\n",
    "documents = reader.load_data()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split document into chunks\n",
    "splitter = SentenceSplitter(chunk_size=200, chunk_overlap=0)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 0.3 Arize Phoenix setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "\tproject_name=\"default\",\n",
    "\tendpoint=\"https://app.phoenix.arize.com/s/tejas-er/v1/traces\",\n",
    "\tauto_instrument=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Part 1—Simple RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the necessary libraries\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup LLM and Embedding model\n",
    "Settings.llm = OpenAI(\n",
    "\tmodel=\"gpt-4.1-mini\", api_key=OPENAI_API_KEY, temperature=0, verbose=False\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "\tmodel=\"text-embedding-3-small\", api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 1.1 Inspecting the vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vector store to access it directly\n",
    "vector_store = vector_index.vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding dictionary and node dictionary\n",
    "embedding_dict = vector_store.data.embedding_dict\n",
    "node_dict = vector_store.data.text_id_to_ref_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of embeddings: {len(embedding_dict)}\")\n",
    "print(f\"Number of node references: {len(node_dict)}\")\n",
    "print(f\"Embedding dimension: {len(list(embedding_dict.values())[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 1.2 Asking question to RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query vector store\n",
    "response = query_engine.query(\"Who is Lareina Yee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 1.3 Checking if the response makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out relevant source nodes\n",
    "print(\"Relevant source nodes:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, node in enumerate(response.source_nodes):\n",
    "\tprint(f\"Node {idx + 1}\")\n",
    "\tprint(f\"Score: {node.score}\")\n",
    "\tprint(f\"Text: {node.text}\")\n",
    "\tprint(f\"Metadata: {node.metadata}\")\n",
    "\tprint(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Part 2 - Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 2.1 Setup vector and summary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 2.2 Create vector query engine and summary query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "\tresponse_mode=\"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector query engine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine(\n",
    "\tresponse_mode=ResponseMode.COMPACT, use_async=True, top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 2.3 Convert the vectors and summary query engine into tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "\tquery_engine=summary_query_engine,\n",
    "\tdescription=\"Useful when you need to answer questions related to the summary of the document.\",\n",
    ")\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "\tquery_engine=vector_query_engine,\n",
    "\tdescription=\"Useful when you need to answer specific questions from the document.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 2.4 Create a superset query to manage both query engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "\tselector=LLMSingleSelector.from_defaults(),\n",
    "\tquery_engine_tools=[summary_tool, vector_tool],\n",
    "\tverbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 2.5 Testing if routing works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who is Lareina yee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 2.6 Convert the query engine into a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tool wrapper around\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "\tquery_engine=query_engine,\n",
    "\tname=\"state_of_ai_rag_tool\",\n",
    "\tdescription=\"Answer's question on the McKinsey 2025 State of AI Report\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 2.7 Define system prompt for final Agentic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert AI assistant trained on the McKinsey report **'The State of AI – March 2025'**.\n",
    "You answer user questions only using the information contained in this report.\n",
    "### Available Tools\n",
    "You have access to two tools:\n",
    "1. **SummaryTool**\n",
    "   - Use this when the query asks for:\n",
    "     - High-level insights or executive summaries\n",
    "     - Trends, survey findings, or general understanding\n",
    "     - Broad takeaways or conceptual analysis\n",
    "   - Example triggers:\n",
    "     - \"Summarize key findings on GenAI adoption\"\n",
    "     - \"What are the main themes of the report?\"\n",
    "2. **VectorTool**\n",
    "   - Use this when the query asks for:\n",
    "     - Specific data, statistics, or exhibit-based evidence\n",
    "     - Organizational practices, metrics, or concrete examples\n",
    "   - Example triggers:\n",
    "     - \"What percentage of companies track AI KPIs?\"\n",
    "     - \"Who is responsible for AI governance in large firms?\"\n",
    "### Tool Selection Logic\n",
    "Before answering:\n",
    "1. Identify whether the query requires **broad synthesis** or **specific factual retrieval**.\n",
    "2. Select and call the correct tool accordingly:\n",
    "   - For high-level or conceptual queries → `use_tool(\"SummaryTool\")`\n",
    "   - For factual, data-based, or detailed queries → `use_tool(\"VectorTool\")`\n",
    "3. Use **only one tool per query** unless explicitly required otherwise.\n",
    "### Response Construction Rules\n",
    "- **Grounding:** Only use information from the McKinsey report.\n",
    "  If the question falls outside the report, respond:\n",
    "  _\"The report does not contain that information.\"_\n",
    "- **Precision:** When citing data, mention specific numbers or exhibit insights when available.\n",
    "- **Clarity:** Use concise, structured paragraphs or bullet points.\n",
    "- **Insight:** Always explain the meaning or implication of findings, not just raw facts.\n",
    "### Output Style\n",
    "- Confident, factual, and grounded in the report.\n",
    "- Well-organized and readable.\n",
    "- Do **not** speculate or generate content outside the report context.\n",
    "### Examples\n",
    "**use_tool(\"SummaryTool\")**\n",
    "- “How are companies restructuring to adopt GenAI?”\n",
    "- “What does the report say about workforce reskilling?”\n",
    "**use_tool(\"VectorTool\")**\n",
    "- “What percentage of companies have a GenAI roadmap?”\n",
    "- “Which departments lead AI strategy execution?”\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "\ttools_or_functions=[query_engine_tool],\n",
    "\tsystem_prompt=system_prompt,\n",
    "\tllm=Settings.llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Lareina Yee according to the document? Where is she mentioned in the document and in what context?\"\n",
    "response = await query_engine_agent.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Part 3- Augment the agent with LlamaHub tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LlamaHub tools\n",
    "from llama_index.tools.brave_search import BraveSearchToolSpec\n",
    "from llama_index.tools.arxiv import ArxivToolSpec\n",
    "from llama_index.tools.wikipedia import WikipediaToolSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up tools\n",
    "arxiv_tool = ArxivToolSpec()\n",
    "arxiv_tools = arxiv_tool.to_tool_list()\n",
    "brave_search_tool = BraveSearchToolSpec(api_key=BRAVE_SEARCH_API_KEY)\n",
    "brave_search_tools = brave_search_tool.to_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_tool = WikipediaToolSpec()\n",
    "wikipedia_tools = wikipedia_tool.to_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_tools = [query_engine_tool]\n",
    "enhanced_tools.extend(brave_search_tools)\n",
    "enhanced_tools.extend(arxiv_tools)\n",
    "enhanced_tools.extend(wikipedia_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### 3.1 Enhanced agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "You are an AI research assistant with access to:\n",
    "1. The state of ai report 2025 by McKinsey\n",
    "2. Web search using brave search\n",
    "3. Arxiv research paper search\n",
    "4. Wikipedia search\n",
    "Use these tools to provide comprehensive, well-researched and accurate answers to user's questions.  When discussing AI trends, combine insight from the mckinsey report with recent research and web findings.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_agent = AgentWorkflow.from_tools_or_functions(\n",
    "\ttools_or_functions=enhanced_tools, llm=Settings.llm, system_prompt=new_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us test the enhanced agent\n",
    "question_1 = \"\"\"According to the mckinsey report, what are the main organizational changes companies are making for AI agents.  Can you please search for recent research papers on AI governance and organizational transformation to provide additional information\"\"\".strip()\n",
    "print(\"Question 1. Organizational changes and governance\")\n",
    "print(\"=\" * 50)\n",
    "response_1 = await enhanced_agent.run(question_1)\n",
    "print(response_1)\n",
    "print(\"*\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"\"\"What does McKinsey report say about workflow redesign for AI implementation? Search Arxiv for papers on business process automation with AI and find current web articles about workflow transformations.\"\"\".strip()\n",
    "print(\"Question 2. Workflow redesign and Implementation\")\n",
    "print(\"=\" * 50)\n",
    "response_2 = await enhanced_agent.run(question_2)\n",
    "print(response_2)\n",
    "print(\"*\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_3 = \"\"\"Based on the McKinsey report, what are the key risks organizations are addressing with gen ai? Can you search the web for recent academic research on AI risk mitigation and compare with the report's findings?\n",
    "\"\"\".strip()\n",
    "print(\"Question 3. Risk management\")\n",
    "print(\"=\" * 50)\n",
    "response_3 = await enhanced_agent.run(question_3)\n",
    "print(response_3)\n",
    "print(\"*\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_4 = \"\"\"\n",
    "Who is Lareina Yee in the McKinsey report and what are her views on AI's workforce impact?\n",
    "After finding the information about her from the document, please:\n",
    "1. search the web using brave search for recent articles, interviews or news about Lareina Yee and her work on AI\n",
    "2. search arxiv for papers about her work on AI and find current web articles about her work on AI, workforce transformation and AI risk mitigation\n",
    "3. provide a comprehensive profile combining the information from all the above sources about her expertise and contributions to AI.\n",
    "\"\"\".strip()\n",
    "print(\"Question 4. Detailed Information about Lareina Yee\")\n",
    "print(\"=\" * 50)\n",
    "response_4 = await enhanced_agent.run(question_4)\n",
    "print(response_4)\n",
    "print(\"*\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
