{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Agentic RAG using LlamaIndex\n",
    "\n",
    "## What is LlamaIndex?\n",
    "- Initially it was developed specifically for a Retrival Augmented Generation use case.  Now it has evolved and is used for AI Agents as well.\n",
    "- Complete toolkit for context-augmented LLM applications.\n",
    "- Main Components of LlamaIndex:\n",
    "    - Data connectors (ingest and format existing data)\n",
    "    - Data Indexes (structure and store data to be consumed by LLM)\n",
    "    - Engines (Chat engine and Query engine)\n",
    "    - Agents (simple tools, helper function to API integrations)\n",
    "    - Observability / Evaluations\n",
    "    - Workflows (event-driven or graph-based system)\n",
    "- What makes it special?\n",
    "    - Easy document parsing using LlamaParse\n",
    "    - Many ready-to-use components\n",
    "    - Simple and clear workflow system\n",
    "    - LlamaHub (3rd party ready-to-use tools)\n",
    "\n",
    "\n",
    "## Why moving away from SmolAgent?\n",
    "- SmolAgent is a minimalistic library to create coding and tool calling agent.\n",
    "- Great for creating simple agents but becomes complicated for a complex or multiple task use case.\n",
    "- Single Agent uses long context and high token as well as prone to hallucination during complex reasoning tasks.\n",
    "- Multiple Agent overcomes the above issues, but the library lacks flexibility, # of out of box tools available and is not scalable.\n",
    "\n",
    "## What is RAG?\n",
    "- It is also known as grounded generation.\n",
    "- RAG is a technique extremely useful for creating chatbots.\n",
    "- It only provides relevant information to LLM to answer the user's query leading to better, faster, cheaper, and more relevant information.\n",
    "\n",
    "## How is it different from Agentic-RAG?\n",
    "- has access to real-time data via tools\n",
    "- can access multiple sources of data\n",
    "- can make independant decisions based on the data\n",
    "- supports deeper reasoning, tool integration, and more\n",
    "\n",
    "## Why create RAG-based Agent\n",
    "- Reduction in hallucination\n",
    "- Better memory management\n",
    "- Updated knowledge base of llm\n",
    "\n",
    "## Using LLM-as-a-judge using LangFuse\n",
    "- Using a large LLM to review the responses generated by the agents and evaluate the quality of the responses.\n",
    "- LangFuse supports Ragas library Evolution metrics out-of-box.\n",
    "\n",
    "### Important Metrics\n",
    "- Hallucinations\n",
    "- Trustworthiness\n",
    "- Relevance\n",
    "- Correctness and completeness\n",
    "- Efficiency (token/time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Part 0: Library used\n",
    "- llama-index\n",
    "- llama-index-vector-stores-chroma\n",
    "- llama-index-embeddings-openai\n",
    "- llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup path\n",
    "from pathlib import Path\n",
    "\n",
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent\n",
    "pdf_path = BASE_DIR / \"data\" / \"the-state-of-ai.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Part 1â€”Simple RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the necessary libraries\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "reader = SimpleDirectoryReader(input_files=[pdf_path])\n",
    "documents = reader.load_data()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split document into chunks\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup LLM and Embedding model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "\tmodel=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 1.1 Inspecting the vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vector store to access it directly\n",
    "vector_store = vector_index.vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding dictionary and node dictionary\n",
    "embedding_dict = vector_store.data.embedding_dict\n",
    "node_dict = vector_store.data.text_id_to_ref_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of embeddings: {len(embedding_dict)}\")\n",
    "print(f\"Number of node references: {len(node_dict)}\")\n",
    "print(f\"Embedding dimension: {len(list(embedding_dict.values())[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 1.2 Asking question to RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query vector store\n",
    "response = query_engine.query(\"Who is Lareina Yee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 1.3 Checking if the response makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out relevant source nodes\n",
    "print(\"Relevant source nodes:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, node in enumerate(response.source_nodes):\n",
    "\tprint(f\"Node {idx + 1}\")\n",
    "\tprint(f\"Score: {node.score}\")\n",
    "\tprint(f\"Text: {node.text}\")\n",
    "\tprint(f\"Metadata: {node.metadata}\")\n",
    "\tprint(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
