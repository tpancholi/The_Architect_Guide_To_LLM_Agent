{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# local RAG pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:39:50.911886Z",
     "start_time": "2025-10-06T13:39:50.451519Z"
    }
   },
   "source": [
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\repos\\python\\The_Architect_Guide_To_LLM_Agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:39:54.772481Z",
     "start_time": "2025-10-06T13:39:54.770474Z"
    }
   },
   "source": [
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:05.291122Z",
     "start_time": "2025-10-06T13:40:05.287008Z"
    }
   },
   "source": [
    "print(f\"Project root set to: {BASE_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: F:\\repos\\python\\The_Architect_Guide_To_LLM_Agent\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:06.822528Z",
     "start_time": "2025-10-06T13:40:06.820045Z"
    }
   },
   "source": [
    "pdf_path = BASE_DIR / \"data\" / \"human_nutrition_text.pdf\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:07.766316Z",
     "start_time": "2025-10-06T13:40:07.761558Z"
    }
   },
   "source": [
    "def download_pdf_requests(\n",
    "\turl: str, dest: Path, timeout: int = 30, max_retries: int = 3\n",
    ") -> None:\n",
    "\t\"\"\"Download a PDF file from URL with progress tracking and error handling.\"\"\"\n",
    "\tdest.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tfor attempt in range(max_retries):\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(url, stream=True, timeout=timeout)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\n",
    "\t\t\tcontent_type = response.headers.get(\"content-type\", \"\").lower()\n",
    "\t\t\tif \"pdf\" not in content_type:\n",
    "\t\t\t\traise ValueError(f\"Invalid content type: {content_type}\")\n",
    "\t\t\ttotal = int(response.headers.get(\"content-length\", 0))\n",
    "\t\t\twith tqdm(\n",
    "\t\t\t\ttotal=total, unit=\"iB\", unit_scale=True, desc=\"Downloading PDF\"\n",
    "\t\t\t) as t:\n",
    "\t\t\t\twith dest.open(\"wb\") as f:\n",
    "\t\t\t\t\tfor chunk in response.iter_content(chunk_size=8192):\n",
    "\t\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\t\tt.update(len(chunk))\n",
    "\t\t\tprint(f\"\\nSuccessfully downloaded PDF to {dest}\")\n",
    "\t\t\treturn\n",
    "\t\texcept requests.exceptions.RequestException as e:\n",
    "\t\t\tprint(f\"Download failed: {e}\")\n",
    "\t\t\tif attempt == max_retries - 1:\n",
    "\t\t\t\traise\n",
    "\t\t\ttime.sleep(2**attempt)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:30.556983Z",
     "start_time": "2025-10-06T13:40:11.777276Z"
    }
   },
   "source": [
    "if not pdf_path.is_file():\n",
    "\tdownload_pdf_requests(\n",
    "\t\t\"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\",\n",
    "\t\tpdf_path,\n",
    "\t)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PDF: 26.9MiB [00:15, 1.77MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully downloaded PDF to F:\\repos\\python\\The_Architect_Guide_To_LLM_Agent\\data\\human_nutrition_text.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:46.655869Z",
     "start_time": "2025-10-06T13:40:46.652276Z"
    }
   },
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "\t\"\"\"Performs minor text formatting.\"\"\"\n",
    "\timport re\n",
    "\n",
    "\tcleaned_text = re.sub(\n",
    "\t\tr\"\\s+\", \" \", text\n",
    "\t)  # Replace multiple whitespace with single space\n",
    "\tcleaned_text = cleaned_text.strip()\n",
    "\treturn cleaned_text"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:48.165144Z",
     "start_time": "2025-10-06T13:40:48.159587Z"
    }
   },
   "source": [
    "import re\n",
    "from typing import Dict, List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def open_and_read_pdf(file_path: Union[str, Path]) -> Union[List[Dict], None]:\n",
    "\t\"\"\"\n",
    "\tOpens a pdf file and reads its content page by page, and collects statistics.\n",
    "\tParameters:\n",
    "\t    file_path (str | Path): The path to the pdf file to be opened and read.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries containing the page number, character count, word count, sentence count, token count, and extracted text for each page.\n",
    "\t\"\"\"\n",
    "\tif not Path(file_path).exists():\n",
    "\t\traise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "\ttry:\n",
    "\t\tdoc = pymupdf.open(file_path)\n",
    "\t\tpages_and_texts = []\n",
    "\t\tfor page_number, page in tqdm(enumerate(doc)):\n",
    "\t\t\ttext = page.get_text()\n",
    "\t\t\tif not text or not text.strip():  # Skip empty pages\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif text and text.strip():\n",
    "\t\t\t\ttext = text_formatter(text)\n",
    "\t\t\t\tsentences = re.split(r\"[.!?]+\", text)  # Simple sentence splitter\n",
    "\t\t\t\tsentence_count = len(\n",
    "\t\t\t\t\t[s for s in sentences if s.strip()]\n",
    "\t\t\t\t)  # Count non-empty sentences\n",
    "\t\t\t\tpages_and_texts.append(\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"page_number\": page_number - 41,\n",
    "\t\t\t\t\t\t\"page_char_count\": len(text),\n",
    "\t\t\t\t\t\t\"page_word_count\": len(text.split()),\n",
    "\t\t\t\t\t\t\"page_sentence_count_raw\": sentence_count,\n",
    "\t\t\t\t\t\t\"page_token_count\": int(len(text) / 4),\n",
    "\t\t\t\t\t\t\"text\": text,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\treturn pages_and_texts\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error reading PDF file: {e}\")\n",
    "\t\treturn None"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T13:40:51.539390Z",
     "start_time": "2025-10-06T13:40:50.846204Z"
    }
   },
   "source": [
    "pages_and_texts = open_and_read_pdf(file_path=pdf_path)\n",
    "if pages_and_texts:\n",
    "\tprint(pages_and_texts[:2])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [00:00, 1802.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page_number': -41, 'page_char_count': 29, 'page_word_count': 4, 'page_sentence_count_raw': 1, 'page_token_count': 7, 'text': 'Human Nutrition: 2020 Edition'}, {'page_number': -39, 'page_char_count': 308, 'page_word_count': 42, 'page_sentence_count_raw': 1, 'page_token_count': 77, 'text': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame(pages_and_texts)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Chunking Strategy\n",
    "- Fixed size Chunking\n",
    "\n",
    "    - Chunk size is fixed\n",
    "    - usually used with social media where data is huge and unstructured and deep understanding is not important but speed is important\n",
    "\n",
    "    Pros\n",
    "\n",
    "        - Fast process with less overhead\n",
    "\n",
    "    Cons\n",
    "\n",
    "        - Information loss as chunk can be start and stop at any place\n",
    "        - No semantic info is captured\n",
    "- Semantic chunking\n",
    "\n",
    "    - based on similarity of sentences are within a threshold they are part of same chunk\n",
    "\n",
    "    Pros\n",
    "        - maintains coherence\n",
    "        - improves retrieved information\n",
    "    \n",
    "    Cons\n",
    "        - High complexity and compute\n",
    "        - Threshold sensitivity\n",
    "        - Inconsistent chunk size\n",
    "\n",
    "- Structural chunking\n",
    "    \n",
    "    - Can be combined with semantic chunking\n",
    "    - if document structure is there then most intuitive way of chunking\n",
    "\n",
    "    Pros\n",
    "        - fast for well structured document\n",
    "        - consistent and human understandable\n",
    "    \n",
    "    Cons\n",
    "        - chunk size can be unpredictable and might become too large\n",
    "        - large chunk can lead to hallucination\n",
    "\n",
    "- Recursive Chunking\n",
    "\n",
    "    - exploits structure as well as make sure chunk size are manageable\n",
    "    - structural chunking on steroids\n",
    "\n",
    "    Pros\n",
    "        - Avoids splitting halfway, more coherent compared to fixed size chunking\n",
    "\n",
    "    Cons\n",
    "        - computational overhead\n",
    "        - inconsistent chunk size\n",
    "\n",
    "- LLM Chunking\n",
    "\n",
    "    - this applies where apart from semantic chunking everything has failed due to context drift (change in context a lot during single document)\n",
    "    - LLM understands semantic of the complete document and does chunking based on its logic\n",
    "\n",
    "    Pros\n",
    "        - high semantic accuracy\n",
    "        - good for document with rapid context change, unstructured text\n",
    "    \n",
    "    Cons\n",
    "        - computationally expensive\n",
    "        - context window limitation\n",
    "        - stochastic output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Engineer decision thought process\n",
    "- Use fixed size chunking when simplicity and speed matters then perfect coherence\n",
    "- Document or medical data then use structured chunking + recursive chunking\n",
    "- transcript from debate, video, meeting where timestamp might not be there then use semantic chunking\n",
    "- if all above fail then use llm chunking **try to avoid as much as possible** for large data.\n",
    "\n",
    "- For example\n",
    "- **Legal domain** - structured & recursive\n",
    "- **Finance domain** - fixed -> semantic\n",
    "- **Healthcare Data** - structured & recursive\n",
    "- **Education domain** - Semantic -> LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizuara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
