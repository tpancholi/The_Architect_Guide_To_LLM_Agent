{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RAG chunking strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tejaspancholi/Developer/python/vizuara/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: /Users/tejaspancholi/Developer/python/vizuara\n"
     ]
    }
   ],
   "source": [
    "print(f\"Project root set to: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = BASE_DIR / \"data\" / \"human_nutrition_text.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_requests(\n",
    "\turl: str, dest: Path, timeout: int = 30, max_retries: int = 3\n",
    ") -> None:\n",
    "\t\"\"\"Download a PDF file from URL with progress tracking and error handling.\"\"\"\n",
    "\tdest.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tfor attempt in range(max_retries):\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(url, stream=True, timeout=timeout)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\n",
    "\t\t\tcontent_type = response.headers.get(\"content-type\", \"\").lower()\n",
    "\t\t\tif \"pdf\" not in content_type:\n",
    "\t\t\t\traise ValueError(f\"Invalid content type: {content_type}\")\n",
    "\t\t\ttotal = int(response.headers.get(\"content-length\", 0))\n",
    "\t\t\twith tqdm(\n",
    "\t\t\t\ttotal=total, unit=\"iB\", unit_scale=True, desc=\"Downloading PDF\"\n",
    "\t\t\t) as t:\n",
    "\t\t\t\twith dest.open(\"wb\") as f:\n",
    "\t\t\t\t\tfor chunk in response.iter_content(chunk_size=8192):\n",
    "\t\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\t\tt.update(len(chunk))\n",
    "\t\t\tprint(f\"\\nSuccessfully downloaded PDF to {dest}\")\n",
    "\t\t\treturn\n",
    "\t\texcept requests.exceptions.RequestException as e:\n",
    "\t\t\tprint(f\"Download failed: {e}\")\n",
    "\t\t\tif attempt == max_retries - 1:\n",
    "\t\t\t\traise\n",
    "\t\t\ttime.sleep(2**attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pdf_path.is_file():\n",
    "\tdownload_pdf_requests(\n",
    "\t\t\"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\",\n",
    "\t\tpdf_path,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "\t\"\"\"Performs minor text formatting.\"\"\"\n",
    "\timport re\n",
    "\n",
    "\tcleaned_text = re.sub(\n",
    "\t\tr\"\\s+\", \" \", text\n",
    "\t)  # Replace multiple whitespace with single space\n",
    "\tcleaned_text = cleaned_text.strip()\n",
    "\treturn cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def open_and_read_pdf(file_path: Union[str, Path]) -> Union[List[Dict], None]:\n",
    "\t\"\"\"\n",
    "\tOpens a pdf file and reads its content page by page, and collects statistics.\n",
    "\tParameters:\n",
    "\t    file_path (str | Path): The path to the pdf file to be opened and read.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries containing the page number, character count, word count, sentence count, token count, and extracted text for each page.\n",
    "\t\"\"\"\n",
    "\tif not Path(file_path).exists():\n",
    "\t\traise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "\ttry:\n",
    "\t\tdoc = pymupdf.open(file_path)\n",
    "\t\tpages_and_texts = []\n",
    "\t\tfor page_number, page in tqdm(enumerate(doc)):\n",
    "\t\t\ttext = page.get_text()\n",
    "\t\t\tif not text or not text.strip():  # Skip empty pages\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif text and text.strip():\n",
    "\t\t\t\ttext = text_formatter(text)\n",
    "\t\t\t\tsentences = re.split(r\"[.!?]+\", text)  # Simple sentence splitter\n",
    "\t\t\t\tsentence_count = len(\n",
    "\t\t\t\t\t[s for s in sentences if s.strip()]\n",
    "\t\t\t\t)  # Count non-empty sentences\n",
    "\t\t\t\tpages_and_texts.append(\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"page_number\": page_number - 41,\n",
    "\t\t\t\t\t\t\"page_char_count\": len(text),\n",
    "\t\t\t\t\t\t\"page_word_count\": len(text.split()),\n",
    "\t\t\t\t\t\t\"page_sentence_count_raw\": sentence_count,\n",
    "\t\t\t\t\t\t\"page_token_count\": int(len(text) / 4),\n",
    "\t\t\t\t\t\t\"text\": text,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\treturn pages_and_texts\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error reading PDF file: {e}\")\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [00:00, 1249.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page_number': -41, 'page_char_count': 29, 'page_word_count': 4, 'page_sentence_count_raw': 1, 'page_token_count': 7, 'text': 'Human Nutrition: 2020 Edition'}, {'page_number': -39, 'page_char_count': 308, 'page_word_count': 42, 'page_sentence_count_raw': 1, 'page_token_count': 77, 'text': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pages_and_texts = open_and_read_pdf(file_path=pdf_path)\n",
    "if pages_and_texts:\n",
    "\tprint(pages_and_texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 222,\n",
       "  'page_char_count': 1568,\n",
       "  'page_word_count': 232,\n",
       "  'page_sentence_count_raw': 10,\n",
       "  'page_token_count': 392,\n",
       "  'text': 'There is no good evidence that chronic caffeine exposure increases blood pressure chronically in people without hypertension. Some have hypothesized that caffeine elevates calcium excretion and therefore could potentially harm bones. The scientific consensus at this time is that caffeine minimally affects calcium levels and intake is not associated with any increased risk for osteoporosis or the incidence of fractures in most women. Although the effect of caffeine on calcium excretion is small, postmenopausal women with risk factors for osteoporosis may want to make sure their dietary caffeine intake is low or moderate and not excessive. The Caffeine Myth A diuretic refers to any substance that elevates the normal urine output above that of drinking water. Caffeinated beverages are commonly believed to be dehydrating due to their diuretic effect, but results from scientific studies do not support that caffeinated beverages increase urine output more so than water. This does not mean that consuming caffeinated beverages does not affect urine output, but rather that it does not increase urine output more than water does. Thus, caffeinated beverages are considered a source of hydration similar to water. Sports Drinks Scientific studies under certain circumstances show that consuming sports drinks (instead of plain water) during high- intensity exercise lasting longer than one hour significantly enhances endurance, and some evidence indicates it additionally enhances performance. There is no consistent evidence that 222 | Popular Beverage Choices'},\n",
       " {'page_number': 503,\n",
       "  'page_char_count': 1743,\n",
       "  'page_word_count': 261,\n",
       "  'page_sentence_count_raw': 20,\n",
       "  'page_token_count': 435,\n",
       "  'text': 'high levels of activity (about one hour of exercise per day). Moreover, most members eat breakfast every day, watch fewer than ten hours of television per week, and weigh themselves at least once per week. About half of them lost weight on their own, and the other half used some type of weight-loss program. In most scientific studies successful weight loss is accomplished only by changing the diet and by increasing physical activity. Doing one without the other limits the amount of weight lost and the length of time that weight loss is sustained. On an individual level it is quite possible to achieve successful weight loss, as over ten thousand Americans can attest. Moreover, losing as little as 10 percent of your body weight can significantly improve health and reduce disease risk.4 You do not have to be overweight or obese to reap benefits from eating a healthier diet and increasing physical activity as both provide numerous benefits beyond weight loss and maintenance. Evidence-Based Dietary Recommendations The 2015 Dietary Guidelines for Americans offers specific, evidence-based recommendations for dietary changes aimed at keeping calorie intake in balance with physical activity, which is key for weight management. These recommendations include: Follow a healthy eating pattern that accounts for all foods and beverages within an appropriate calorie level that includes: 4. Clinical Guidelines on the Identification, Evaluation, and Treatment of Overweight and Obesity in Adults: The Evidence Report. National Heart, Lung, and Blood Institute. 1998, 51S–210S. http://www.ncbi.nlm.nih.gov/ books/NBK2003/. Accessed September 22, 2017. Dietary, Behavioral, and Physical Activity Recommendations for Weight Management | 503'},\n",
       " {'page_number': 1164,\n",
       "  'page_char_count': 1679,\n",
       "  'page_word_count': 237,\n",
       "  'page_sentence_count_raw': 15,\n",
       "  'page_token_count': 419,\n",
       "  'text': 'Flashcard Images Note: Most images in the flashcards have been in the flashcards. Please see the H5P source files for more information. For complex images, please see below: 1. Beta-Cryptoxanthin reused “Corn” by charlesdeluvio / Unsplash License; “Lemon” by ohleighann / Unsplash License; “Pepper” by mukeshsankhyaan / Unsplash License 2. Bioavailability reused “Apple Red Fruit Food Bitten Eaten” by Clker-Free-Vector-Images / Pixabay License; “Digestive system without labels” by Mariana Ruiz / Public Domain 3. Carbohydrates reused “Bread” by Jack7 / Public Domain; reused “The Macronutrients: Carbohydrates, Lipids, Protein, and Water” by Medicine LibreTexts / Attribution-ShareAlike 4. Fiber reused “Apple Green Fruit” by OpenClipart-Vectors / Pixabay License; “Beans Legume Food” by JanNijman / Pixabay License; “Wheat-kernel nutrition” by Jkwchui / CC BY-SA 3.0 5. Fiber reused “Wheat Bran” by Alistair1978 / CC BY-SA 2.5 6. Free Radicals reused “Antioxidants Role” by Allison Calabrese / Attribution – Sharealike 7. Glucagon reused “Weightlifter Gym Tool Athlete” by mohamed_hassan / Pixabay License; “Ice Cream Chocolate Cone Dessert” by Snoy_My / Pixabay License; “Liver Biology Medical Anatomy” by LJNovaScotia / Pixabay License; “Pancreas Organ Anatomy” by zachvanstone8 / Pixabay License 8. Hazard Analysis Critical Control Points reused “HACCP” by USDAgov / Public Domain; “20120106-OC-AMW-0680” by USDAgov / Public Domain 9. Hyperlipidemia reused “Osmotic pressure on blood cells diagram” by Mariana Ruiz / Public Domain 10. Nutrients reused “The Macronutrients: Carbohydrates, Lipids, Protein, and Water” by Medicine LibreTexts / Attribution- 1164 | Attributions'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>page_number</th><th>page_char_count</th><th>page_word_count</th><th>page_sentence_count_raw</th><th>page_token_count</th><th>text</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>1179.0</td><td>1179.0</td><td>1179.0</td><td>1179.0</td><td>1179.0</td><td>&quot;1179&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>561.27</td><td>1148.06</td><td>176.2</td><td>14.81</td><td>286.62</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>348.9</td><td>529.51</td><td>83.18</td><td>9.41</td><td>132.39</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>-41.0</td><td>15.0</td><td>3.0</td><td>1.0</td><td>3.0</td><td>&quot;(Source: UNICEF, 1986, How to …</td></tr><tr><td>&quot;25%&quot;</td><td>259.0</td><td>764.0</td><td>117.0</td><td>9.0</td><td>191.0</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>561.0</td><td>1207.0</td><td>187.0</td><td>13.0</td><td>301.0</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>862.0</td><td>1577.0</td><td>240.0</td><td>20.0</td><td>394.0</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>1166.0</td><td>2271.0</td><td>393.0</td><td>82.0</td><td>567.0</td><td>&quot;• food insecure with severe hu…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 7)\n",
       "┌────────────┬─────────────┬──────────────┬──────────────┬─────────────┬─────────────┬─────────────┐\n",
       "│ statistic  ┆ page_number ┆ page_char_co ┆ page_word_co ┆ page_senten ┆ page_token_ ┆ text        │\n",
       "│ ---        ┆ ---         ┆ unt          ┆ unt          ┆ ce_count_ra ┆ count       ┆ ---         │\n",
       "│ str        ┆ f64         ┆ ---          ┆ ---          ┆ w           ┆ ---         ┆ str         │\n",
       "│            ┆             ┆ f64          ┆ f64          ┆ ---         ┆ f64         ┆             │\n",
       "│            ┆             ┆              ┆              ┆ f64         ┆             ┆             │\n",
       "╞════════════╪═════════════╪══════════════╪══════════════╪═════════════╪═════════════╪═════════════╡\n",
       "│ count      ┆ 1179.0      ┆ 1179.0       ┆ 1179.0       ┆ 1179.0      ┆ 1179.0      ┆ 1179        │\n",
       "│ null_count ┆ 0.0         ┆ 0.0          ┆ 0.0          ┆ 0.0         ┆ 0.0         ┆ 0           │\n",
       "│ mean       ┆ 561.27      ┆ 1148.06      ┆ 176.2        ┆ 14.81       ┆ 286.62      ┆ null        │\n",
       "│ std        ┆ 348.9       ┆ 529.51       ┆ 83.18        ┆ 9.41        ┆ 132.39      ┆ null        │\n",
       "│ min        ┆ -41.0       ┆ 15.0         ┆ 3.0          ┆ 1.0         ┆ 3.0         ┆ (Source:    │\n",
       "│            ┆             ┆              ┆              ┆             ┆             ┆ UNICEF,     │\n",
       "│            ┆             ┆              ┆              ┆             ┆             ┆ 1986, How   │\n",
       "│            ┆             ┆              ┆              ┆             ┆             ┆ to …        │\n",
       "│ 25%        ┆ 259.0       ┆ 764.0        ┆ 117.0        ┆ 9.0         ┆ 191.0       ┆ null        │\n",
       "│ 50%        ┆ 561.0       ┆ 1207.0       ┆ 187.0        ┆ 13.0        ┆ 301.0       ┆ null        │\n",
       "│ 75%        ┆ 862.0       ┆ 1577.0       ┆ 240.0        ┆ 20.0        ┆ 394.0       ┆ null        │\n",
       "│ max        ┆ 1166.0      ┆ 2271.0       ┆ 393.0        ┆ 82.0        ┆ 567.0       ┆ • food      │\n",
       "│            ┆             ┆              ┆              ┆             ┆             ┆ insecure    │\n",
       "│            ┆             ┆              ┆              ┆             ┆             ┆ with severe │\n",
       "│            ┆             ┆              ┆              ┆             ┆             ┆ hu…         │\n",
       "└────────────┴─────────────┴──────────────┴──────────────┴─────────────┴─────────────┴─────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame(pages_and_texts)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2732ee0",
   "metadata": {},
   "source": [
    "## Method 1: Fixed size chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500) -> List[str]:\n",
    "\t\"\"\"Splits text into chunks of specified size with overlap.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    chunk_size (int): The size of each chunk in words.\n",
    "\tReturns:\n",
    "\t    List[str]: A list of text chunks.\n",
    "\t\"\"\"\n",
    "\tchunks = []\n",
    "\tcurrent_chunk = \"\"\n",
    "\twords = text.split()\n",
    "\n",
    "\tfor word in words:\n",
    "\t\tif len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "\t\t\tcurrent_chunk += word + \" \"\n",
    "\t\telse:\n",
    "\t\t\tchunks.append(current_chunk.strip())\n",
    "\t\t\tcurrent_chunk = word + \" \"\n",
    "\n",
    "\tif current_chunk:\n",
    "\t\tchunks.append(current_chunk.strip())\n",
    "\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_pdf_pages(pages_and_texts: list, chunk_size: int = 500) -> List[Dict]:\n",
    "\t\"\"\"Chunks the text of each page into smaller segments.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of dictionaries containing page information and text.\n",
    "\t    chunk_size (int): The size of each chunk in words.\n",
    "\tReturns:\n",
    "\t    List[Dict]: A list of dictionaries with chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tchunked_data = []\n",
    "\n",
    "\tfor page in pages_and_texts:\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = chunk_text(text, chunk_size)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tchunked_data.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a9a7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_pages = chunk_pdf_pages(pages_and_texts, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a3394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 3321\n",
      "25th chunk (page -28): Introduction University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program 515 Fat-Soluble Vitamins University of Hawai‘i at Mānoa Food Science and Human Nutritio...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks created: {len(chunked_pages)}\")\n",
    "print(\n",
    "\tf\"25th chunk (page {chunked_pages[24]['page_number']}): {chunked_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982ac431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Generate k scattered indices over range n with some jitter.\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2605ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {c['chunk_token_count']}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0af832d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_chunks(\n",
    "\tpages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    chunk_size (int): Size of each text chunk.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\t# Chunk the text from each page\n",
    "\tall_chunks = []\n",
    "\tall_chunks = chunk_pdf_pages(pages_and_texts, chunk_size)\n",
    "\tif not all_chunks:\n",
    "\t\tprint(\"No chunks available to display.\")\n",
    "\t\treturn\n",
    "\tindices = _scattered_indices(len(all_chunks), k)\n",
    "\tprint(\n",
    "\t\tf\"Showing {len(indices)} scattered random chunks out of {len(all_chunks)} total chunks:\\n\"\n",
    "\t)\n",
    "\tfor i, idx in enumerate(indices, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(all_chunks[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dfc1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 5 scattered random chunks out of 3321 total chunks:\n",
      "\n",
      "#1\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p-9 - idx 0  | chars 290 - words 49 - tokens 72                                            |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Skylar Hara Skylar Hara is an undergraduate student student in the Tropical Agriculture and the  │\n",
      "│ Environment program at the University of Hawai‘i at Mānoa. She has a growing love for plants and │\n",
      "│ hopes to go to graduate school to conduct research in the future. About the Contributors |       │\n",
      "│ xxxiii                                                                                           │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#2\n",
      "┌─────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p198 - idx 0  | chars 497 - words 71 - tokens 124                                         |\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ foods. Fresh and frozen foods are better sources of potassium than canned. Learning Activities  │\n",
      "│ Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER)      │\n",
      "│ textbook features interactive learning activities. These activities are available in the web-   │\n",
      "│ based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or │\n",
      "│ Open Document). Learning activities may be used across various mobile devices, however, for the │\n",
      "│ best user experience it is                                                                      │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#3\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p579 - idx 0  | chars 496 - words 77 - tokens 124                                          |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Image by Allison Calabrese / CC BY 4.0 Folate is especially essential for the growth and         │\n",
      "│ specialization of cells of the central nervous system. Children whose mothers were folate-       │\n",
      "│ deficient during pregnancy have a higher risk of neural-tube birth defects. Folate deficiency is │\n",
      "│ causally linked to the development of spina bifida, a neural-tube defect that occurs when the    │\n",
      "│ spine does not completely enclose the spinal cord. Spina bifida can lead to many physical and    │\n",
      "│ mental disabilities (Figure 9.18                                                                 │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#4\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p882 - idx 2  | chars 300 - words 34 - tokens 75                                           |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ best user experience it is strongly 9. Lead Exposure: Tips to Protect Your Child. Mayo           │\n",
      "│ Foundation for Medical Education and Research. https://www.mayoclinic.org/diseases-              │\n",
      "│ conditions/lead- poisoning/in-depth/lead-exposure/art-20044627. Updated March 12, 2015. Accessed │\n",
      "│ December 5, 2017. 882 | Childhood                                                                │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#5\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p1117 - idx 1  | chars 456 - words 57 - tokens 114                                         |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ triglycerides greater than 150 mg/dL; high density lipoproteins (HDL) lower than 40 mg/dL;       │\n",
      "│ systolic blood pressure above 100 mmHg, or diastolic above 85 mmHg; fasting blood-glucose levels │\n",
      "│ greater than 100 mg/dL.16 The IDF estimates that between 20 and 16. The IDF Consensus Worldwide  │\n",
      "│ Definition of the Metabolic Syndrome. International Diabetes Federation.https://www.idf.org/our- │\n",
      "│ activities/ advocacy-awareness/resources-and-tools/ Threats to Health | 1117                     │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert pages_and_texts is not None\n",
    "show_random_chunks(pages_and_texts, chunk_size=500, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189352f",
   "metadata": {},
   "source": [
    "### here you might have seen some chunk are smaller than 500 even though we have mentioned as chunk size as 500, its because the processes is happening at page level and it can happen once a couple of chunk are done at the page, rest of text is smaller than 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e744d6",
   "metadata": {},
   "source": [
    "## Method 2: Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f1cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec548b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a609cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk_text(\n",
    "\ttext: str, similarity_threshold: float = 0.8, max_tokens: int = 500\n",
    ") -> list:\n",
    "\t\"\"\"Splits text into semantically coherent chunks based on sentence embeddings.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    similarity_threshold (float): Cosine similarity threshold to determine chunk boundaries.\n",
    "\t    max_tokens (int): Maximum number of tokens per chunk.\n",
    "\tReturns:\n",
    "\t    list: A list of semantically coherent text chunks.\n",
    "\t\"\"\"\n",
    "\tsentences = nltk.sent_tokenize(text)\n",
    "\tif not sentences:\n",
    "\t\treturn []\n",
    "\n",
    "\t# ensure embeddings is a numpy array of shape (n_sentences, dim)\n",
    "\tembeddings = semantic_model.encode(sentences, convert_to_numpy=True)\n",
    "\tif not isinstance(embeddings, np.ndarray):\n",
    "\t\tembeddings = np.array(embeddings)\n",
    "\n",
    "\tchunks = []\n",
    "\tcurrent_chunk = [sentences[0]]\n",
    "\tcurrent_indices = [0]\n",
    "\n",
    "\tfor i in range(1, len(sentences)):\n",
    "\t\tcurrent_embedding = np.mean(embeddings[current_indices], axis=0)\n",
    "\t\tnext_embedding = embeddings[i]\n",
    "\t\tsim = float(\n",
    "\t\t\tcosine_similarity(\n",
    "\t\t\t\tcurrent_embedding.reshape(1, -1), next_embedding.reshape(1, -1)\n",
    "\t\t\t)[0, 0]\n",
    "\t\t)\n",
    "\n",
    "\t\tchunk_token_count = len(\" \".join(current_chunk)) // 4\n",
    "\n",
    "\t\tif sim >= similarity_threshold and chunk_token_count < max_tokens:\n",
    "\t\t\tcurrent_chunk.append(sentences[i])\n",
    "\t\t\tcurrent_indices.append(i)\n",
    "\t\telse:\n",
    "\t\t\tchunks.append(\" \".join(current_chunk))\n",
    "\t\t\tcurrent_chunk = [sentences[i]]\n",
    "\t\t\tcurrent_indices = [i]\n",
    "\tif current_chunk:\n",
    "\t\tchunks.append(\" \".join(current_chunk))\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fee55db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk_pdf_pages(\n",
    "\tpages_and_texts: list, similarity_threshold: float = 0.8, max_tokens: int = 500\n",
    ") -> list[dict]:\n",
    "\t\"\"\"Chunks the text of each page into semantically coherent segments.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of dictionaries containing page information and text.\n",
    "\t    similarity_threshold (float): Cosine similarity threshold to determine chunk boundaries.\n",
    "\t    max_tokens (int): Maximum number of tokens per chunk.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with semantically chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tall_chunks = []\n",
    "\n",
    "\tfor page in tqdm(pages_and_texts, desc=\"Semantic chunking pages\"):\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = semantic_chunk_text(text, similarity_threshold, max_tokens)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tall_chunks.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7feba889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tejaspancholi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Semantic chunking pages: 100%|██████████| 1179/1179 [00:12<00:00, 93.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "semantic_chunk_pages = semantic_chunk_pdf_pages(\n",
    "\tpages_and_texts, similarity_threshold=0.75, max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e93403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks created: 12027\n",
      "25th semantic chunk (page -29): Alcohol Introduction University of Hawai‘i at Mānoa Food Science and Human Nutrition Program and Human Nutrition Program 431 Alcohol Metabolism University of Hawai‘i at Mānoa Food Science and Human Nu...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total semantic chunks created: {len(semantic_chunk_pages)}\")\n",
    "print(\n",
    "\tf\"25th semantic chunk (page {semantic_chunk_pages[24]['page_number']}): {semantic_chunk_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0e6b1",
   "metadata": {},
   "source": [
    "### Number of chunks have drastically increased is because there is very limited similiarity meaning between pages or paras, leading to smaller chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a86d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Evenely spaced anchors + random jitters + indices scattered across [0,n-1]\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7b431f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\tapprox_tokens = c.get(\"chunk_token_count\", len(c[\"chunk_text\"]) / 4)\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {round(approx_tokens, 2)}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64d3f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_semantic_random_chunks(\n",
    "\tsemantic_chunked_pages: list[dict], k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the semantic chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\tn = len(semantic_chunked_pages)\n",
    "\tif n == 0:\n",
    "\t\tprint(\"No semantic chunks available to display.\")\n",
    "\t\treturn\n",
    "\tidxs = _scattered_indices(n, k)\n",
    "\tprint(\n",
    "\t\tf\"Showing {len(idxs)} scattered random semantic chunks out of {n} total chunks:\\n\"\n",
    "\t)\n",
    "\tfor i, idx in enumerate(idxs, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(semantic_chunked_pages[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb7e48ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 5 scattered random semantic chunks out of 12027 total chunks:\n",
      "\n",
      "#1\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p56 - idx 7  | chars 197 - words 31 - tokens 49                                            |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Observing the connection between the beverage and longevity, Dr. Elie Metchnikoff began his      │\n",
      "│ research on beneficial bacteria and the longevity of life that led to his book, The Prolongation │\n",
      "│ of Life.                                                                                         │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#2\n",
      "┌────────────────────────────────────────────────────────┐\n",
      "| Chunk p232 - idx 8  | chars 54 - words 10 - tokens 13  |\n",
      "├────────────────────────────────────────────────────────┤\n",
      "│ As a result, the liver rapidly converts it to glucose. │\n",
      "└────────────────────────────────────────────────────────┘\n",
      "\n",
      "#3\n",
      "┌─────────────────────────────────────────────────────┐\n",
      "| Chunk p509 - idx 12  | chars 28 - words 3 - tokens 7|\n",
      "├─────────────────────────────────────────────────────┤\n",
      "│ Pediatrics, 120(4), e869–79.                        │\n",
      "└─────────────────────────────────────────────────────┘\n",
      "\n",
      "#4\n",
      "┌────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p963 - idx 1  | chars 384 - words 54 - tokens 96                                         |\n",
      "├────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Table 16.2 The Recommended Protein Intakes for Individuals Group Protein Intake (g/kg body     │\n",
      "│ weight) Most adults 0.8 Endurance athletes 1.2 to 1.4 Vegetarian endurance athletes 1.3 to 1.5 │\n",
      "│ Strength athletes 1.6 to 1.7 Vegetarian strength athletes 1.7 to 1.8 Source: Dietary Reference │\n",
      "│ Intakes, 2002 ACSM/ADA/Dietitians of Canada Position Statement: Nutrition & Athletic           │\n",
      "│ Performance, 2001.                                                                             │\n",
      "└────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#5\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p1110 - idx 7  | chars 132 - words 24 - tokens 33                                      |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ This condition occurs when the body uses fats and not glucose to make energy, resulting in a │\n",
      "│ build-up of ketone bodies in the blood.                                                      │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert semantic_chunk_pages is not None\n",
    "show_semantic_random_chunks(semantic_chunk_pages, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b2c9c",
   "metadata": {},
   "source": [
    "## Method 3 - Recursive Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626ed10",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "- if chunk is smaller than `max_chunk_size`, its chunked as is\n",
    "- if its larger, it will try to split by `\\n\\n` which is usually between two sections\n",
    "- if that also does not work it will try to split by `\\n` which is usually a para\n",
    "-  if para is still too large, its split by sentence\n",
    "- repeat the process recursively\n",
    "- *note* the list of separator can be changed, by supplying it during method invocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ab58e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca23d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunk_text(\n",
    "\ttext: str, max_chunk_size: int = 1000, min_chunk_size: int = 100\n",
    ") -> list:\n",
    "\t\"\"\"Recursively chunks text into smaller segments based on size constraints.\n",
    "\tTries splitting by sections, then newlines, then sentences.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    max_chunk_size (int): Maximum size of each chunk in characters.\n",
    "\t    min_chunk_size (int): Minimum size of each chunk in characters.\n",
    "\tReturns:\n",
    "\t    list: A list of text chunks.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef split_chunk(chunk: str) -> list:\n",
    "\t\tif len(chunk) <= max_chunk_size:\n",
    "\t\t\treturn [chunk.strip()]\n",
    "\t\t# first try splitting by sections (double newlines)\n",
    "\t\tsections = chunk.split(\"\\n\\n\")\n",
    "\t\tif len(sections) > 1:\n",
    "\t\t\tresult = []\n",
    "\t\t\tfor section in sections:\n",
    "\t\t\t\tif section.strip():\n",
    "\t\t\t\t\tresult.extend(split_chunk(section.strip()))\n",
    "\t\t\treturn result\n",
    "\t\t# next try splitting by single newlines\n",
    "\t\tsections = chunk.split(\"\\n\")\n",
    "\t\tif len(sections) > 1:\n",
    "\t\t\tresult = []\n",
    "\t\t\tfor section in sections:\n",
    "\t\t\t\tif section.strip():\n",
    "\t\t\t\t\tresult.extend(split_chunk(section.strip()))\n",
    "\t\t\treturn result\n",
    "\t\t# finally split by sentences\n",
    "\t\tsentences = nltk.sent_tokenize(chunk)\n",
    "\t\tchunks, current_chunk, current_size = [], [], 0\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tif current_size + len(sentence) > max_chunk_size:\n",
    "\t\t\t\tif current_chunk:\n",
    "\t\t\t\t\tchunks.append(\" \".join(current_chunk).strip())\n",
    "\t\t\t\tcurrent_chunk, current_size = [sentence], len(sentence)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_chunk.append(sentence)\n",
    "\t\t\t\tcurrent_size += len(sentence)\n",
    "\t\tif current_chunk:\n",
    "\t\t\tchunks.append(\" \".join(current_chunk).strip())\n",
    "\t\treturn chunks\n",
    "\n",
    "\treturn split_chunk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a98884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunk_pdf_pages(\n",
    "\tpages_and_texts: list, max_chunk_size: int = 1000, min_chunk_size: int = 100\n",
    ") -> list[dict]:\n",
    "\t\"\"\"Chunks the text of each page into smaller segments using recursive chunking.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of dictionaries containing page information and text.\n",
    "\t    max_chunk_size (int): Maximum size of each chunk in characters.\n",
    "\t    min_chunk_size (int): Minimum size of each chunk in characters.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with recursively chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tall_chunks = []\n",
    "\n",
    "\tfor page in tqdm(pages_and_texts, desc=\"Recursive chunking pages\"):\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = recursive_chunk_text(text, max_chunk_size, min_chunk_size)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tall_chunks.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c63bc79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recursive chunking pages: 100%|██████████| 1179/1179 [00:00<00:00, 14248.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recursive chunks created: 1949\n",
      "25th recursive chunk (page -17): Preface UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM ‘A‘ohe pau ka ‘ike i ka hālau ho‘okahi Knowledge isn’t taught in all one place This open acc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recursive_chunked_pages = recursive_chunk_pdf_pages(\n",
    "\tpages_and_texts, max_chunk_size=1000, min_chunk_size=100\n",
    ")\n",
    "print(f\"Total recursive chunks created: {len(recursive_chunked_pages)}\")\n",
    "print(\n",
    "\tf\"25th recursive chunk (page {recursive_chunked_pages[24]['page_number']}): {recursive_chunked_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7593ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Generate k scattered indices over range n with some jitter.\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out\n",
    "\n",
    "\n",
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\tapprox_tokens = c.get(\"chunk_token_count\", len(c[\"chunk_text\"]) / 4)\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {round(approx_tokens, 2)}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])\n",
    "\n",
    "\n",
    "def show_recursive_random_chunks(\n",
    "\trecursive_chunked_pages: list[dict], k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the recursive chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\tn = len(recursive_chunked_pages)\n",
    "\tif n == 0:\n",
    "\t\tprint(\"No recursive chunks available to display.\")\n",
    "\t\treturn\n",
    "\tidxs = _scattered_indices(n, k)\n",
    "\tprint(\n",
    "\t\tf\"Showing {len(idxs)} scattered random recursive chunks out of {n} total chunks:\\n\"\n",
    "\t)\n",
    "\tfor i, idx in enumerate(idxs, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(recursive_chunked_pages[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2000aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 5 scattered random recursive chunks out of 1949 total chunks:\n",
      "\n",
      "#1\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p-14 - idx 0  | chars 584 - words 89 - tokens 146                                          |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Alan Titchenal Dr. Titchenal received a PhD in nutrition from the University of California at    │\n",
      "│ Davis with emphasis on exercise physiology and physiological chemistry. His work at the          │\n",
      "│ University of Hawai‘i at Mānoa has focused on the broad areas of nutrition and human performance │\n",
      "│ and translation of nutrition science for public consumption. This has included the “Got          │\n",
      "│ Nutrients?” project that provides daily messages on topics related to nutrition, fitness, and    │\n",
      "│ health and the publication of over 600 articles in the Honolulu Star- Advertiser newspaper.      │\n",
      "│ xxviii | About the Contributors                                                                  │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#2\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p195 - idx 1  | chars 772 - words 123 - tokens 193                                         |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Other Functions of Potassium in the Body Nerve impulse involves not only sodium, but also        │\n",
      "│ potassium. A nerve impulse moves along a nerve via the movement of sodium ions into the cell. To │\n",
      "│ end the impulse, potassium ions rush out of the nerve cell, thereby decreasing the positive      │\n",
      "│ charge inside the nerve cell. This diminishes the stimulus. To restore the original              │\n",
      "│ concentrations of ions between the intracellular and extracellular fluid, the sodium- potassium  │\n",
      "│ pump transfers sodium ions out in exchange for potassium ions in. On completion of the restored  │\n",
      "│ ion concentrations, a nerve cell is now ready to receive the next impulse. Similarly, in muscle  │\n",
      "│ cells potassium is involved in restoring the normal membrane potential and ending the muscle     │\n",
      "│ contraction. Potassium | 195                                                                     │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#3\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p560 - idx 0  | chars 423 - words 68 - tokens 105                                          |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Image by Casimir Funk (1914) / No known copyright restrictions Dietary Reference Intakes The     │\n",
      "│ RDAs and ULs for different age groups for thiamin are listed in Table 9.13 “Dietary Reference    │\n",
      "│ Intakes for Thiamin”. There is no UL for thiamin because there has not been any reports on       │\n",
      "│ toxicity when excess amounts are consumed from food or supplements. Table 9.13 Dietary Reference │\n",
      "│ Intakes for Thiamin 560 | Water-Soluble Vitamins                                                 │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#4\n",
      "┌─────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p861 - idx 1  | chars 735 - words 118 - tokens 183                                        |\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Iron-deficiency anemia causes a number of problems including weakness, pale skin, shortness of  │\n",
      "│ breath, and irritability. It can also result in intellectual, behavioral, or motor problems. In │\n",
      "│ infants and toddlers, iron-deficiency anemia can occur as young children are weaned from iron-  │\n",
      "│ rich foods, such as breast milk and iron-fortified formula. They begin to eat solid foods that  │\n",
      "│ may not provide enough of this nutrient. As a result, their iron stores become diminished at a  │\n",
      "│ time when this nutrient is critical for brain growth and development. 9. Ervin, R. B., Kit, B.  │\n",
      "│ K., Carroll, M. D., & Ogden, C. L. (2012). Consumption of added sugar among U.S. children and   │\n",
      "│ adolescents, 2005-2008. NCHS data brief, (87), 1–8. Toddler Years | 861                         │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#5\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p1111 - idx 0  | chars 704 - words 124 - tokens 176                                        |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Image by Allison Calabrese / CC BY 4.0 Type 2 Diabetes The other 90 to 95 percent of diabetes    │\n",
      "│ cases are Type 2 diabetes. Type 2 diabetes is defined as a metabolic disease of insulin          │\n",
      "│ insufficiency, but it is also caused by muscle, liver, and fat cells no longer responding to the │\n",
      "│ insulin in the body (Figure 18.4 “Healthy Individuals and Type 2 Diabetes” . In brief, cells in  │\n",
      "│ the body have become resistant to insulin and no longer receive the full physiological message   │\n",
      "│ of insulin to take up glucose from the blood. Thus, similar to patients with Type 1 diabetes,    │\n",
      "│ those with Type 2 diabetes also have high blood-glucose levels. Figure 18.4 Healthy Individuals  │\n",
      "│ and Type 2 Diabetes Threats to Health | 1111                                                     │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert recursive_chunked_pages is not None and len(recursive_chunked_pages) > 0\n",
    "show_recursive_random_chunks(recursive_chunked_pages, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22e0ce",
   "metadata": {},
   "source": [
    "## Method 4: Structured based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bfb6c",
   "metadata": {},
   "source": [
    "### How it works\n",
    "- The function looks for `headers` such as Chapter numbers(e.g. CHAPTER 1) or section heading(i.e. 1.1 Introduction)\n",
    "- Every time it finds a header it starts a new chunk till either another heading is reached or max token size limit is exceeded\n",
    "- This preserves logical flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd573c6",
   "metadata": {},
   "source": [
    "### Engineer's decision\n",
    "- Works well with documents which have clear hierarchy(chapter, section, subsection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db999415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# helper function to detect chapter start\n",
    "def _is_chapter_start(text: str) -> bool:\n",
    "\t\"\"\"Detects if a line indicates the start of a new chapter or section.\"\"\"\n",
    "\t# chapter_patterns = [\n",
    "\t#     r'^\\s*CHAPTER\\s+\\d+',  # Matches \"CHAPTER 1\", \"CHAPTER 2\", etc.\n",
    "\t#     r'^\\s*\\d+(\\.\\d+)*\\s+[A-Z][a-zA-Z\\s]*',  # Matches \"1. Introduction\", \"2.1 Background\", etc.\n",
    "\t# ]\n",
    "\t# for pattern in chapter_patterns:\n",
    "\t#     if re.match(pattern, line):\n",
    "\t#         return True\n",
    "\t# return False\n",
    "\treturn re.search(r\"university\\s+of\\s+hawai\", text, flags=re.IGNORECASE) is not None\n",
    "\n",
    "\n",
    "def _guess_title_from_page(text: str) -> str:\n",
    "\t\"\"\"the previous line of 'University of Hawaii' is likely the title\n",
    "\tfalls back to the first ~120 characters\n",
    "\tArgs:\n",
    "\t    text (str): The text of the page.\n",
    "\tReturns:\n",
    "\t    str: The guessed title of the page.\n",
    "\t\"\"\"\n",
    "\tmatch = re.search(r\"university\\s+of\\s+hawai\", text, flags=re.IGNORECASE)\n",
    "\tif match:\n",
    "\t\ttitle = text[: match.start()].strip()\n",
    "\t\ttitle = re.sub(r\"\\s+\", \" \", title.strip())  # Clean up whitespace\n",
    "\t\tif 10 <= len(title) <= 180:\n",
    "\t\t\treturn title\n",
    "\t# fallback to first ~120 characters\n",
    "\ttitle = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\treturn title[:120] if title else \"Untitled\"\n",
    "\n",
    "\n",
    "def chapter_chunk_pdf_pages(pages_and_texts: list) -> list[dict]:\n",
    "\t\"\"\"\n",
    "\tChunks PDF pages into sections based on chapter titles.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with chapter titles and associated pages.\n",
    "\t\"\"\"\n",
    "\tif not pages_and_texts:\n",
    "\t\treturn []\n",
    "\tchapter_starts = []\n",
    "\tfor i, page in enumerate(pages_and_texts):\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tif _is_chapter_start(text):\n",
    "\t\t\tchapter_starts.append(i)\n",
    "\t# if nothing detected, return all pages as a single chunk\n",
    "\tif not chapter_starts:\n",
    "\t\t# No chapters found, return all pages as a single chunk\n",
    "\t\tall_text = \" \".join(page[\"text\"] for page in pages_and_texts).strip()\n",
    "\t\treturn [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"chapter_index\": 0,\n",
    "\t\t\t\t\"chapter_title\": _guess_title_from_page(pages_and_texts[0][\"text\"]),\n",
    "\t\t\t\t\"start_page\": pages_and_texts[0][\"page_number\"],\n",
    "\t\t\t\t\"end_page\": pages_and_texts[-1][\"page_number\"],\n",
    "\t\t\t\t\"chunk_char_count\": len(all_text),\n",
    "\t\t\t\t\"chunk_word_count\": len(all_text.split()),\n",
    "\t\t\t\t\"chunk_token_count\": int(len(all_text) / 4),\n",
    "\t\t\t\t\"chunk_text\": all_text,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t# build chapter ranges (start -> next start - 1)\n",
    "\tchapter_chunks = []\n",
    "\tfor idx, start in enumerate(chapter_starts):\n",
    "\t\tend = (\n",
    "\t\t\tchapter_starts[idx + 1] - 1\n",
    "\t\t\tif idx + 1 < len(chapter_starts)\n",
    "\t\t\telse len(pages_and_texts) - 1\n",
    "\t\t)\n",
    "\t\tif end < start:\n",
    "\t\t\tcontinue  # skip invalid ranges\n",
    "\t\tchapter_pages = pages_and_texts[start : end + 1]\n",
    "\t\tchapter_text = \" \".join(page[\"text\"] for page in chapter_pages).strip()\n",
    "\t\tchapter_title = _guess_title_from_page(chapter_pages[0][\"text\"])\n",
    "\t\tchapter_chunks.append(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"chapter_index\": idx,\n",
    "\t\t\t\t\"chapter_title\": chapter_title,\n",
    "\t\t\t\t\"start_page\": chapter_pages[0][\"page_number\"],\n",
    "\t\t\t\t\"end_page\": chapter_pages[-1][\"page_number\"],\n",
    "\t\t\t\t\"chunk_char_count\": len(chapter_text),\n",
    "\t\t\t\t\"chunk_word_count\": len(chapter_text.split()),\n",
    "\t\t\t\t\"chunk_token_count\": int(len(chapter_text) / 4),\n",
    "\t\t\t\t\"chunk_text\": chapter_text,\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\treturn chapter_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fff3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chapter chunks created: 171\n",
      "1st chapter chunk (pages -39 to -39): Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FI...\n"
     ]
    }
   ],
   "source": [
    "structured_chunk_pages = chapter_chunk_pdf_pages(pages_and_texts)\n",
    "print(f\"Total chapter chunks created: {len(structured_chunk_pages)}\")\n",
    "print(\n",
    "\tf\"1st chapter chunk (pages {structured_chunk_pages[0]['start_page']} to {structured_chunk_pages[0]['end_page']}): {structured_chunk_pages[0]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0ece1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizuara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
