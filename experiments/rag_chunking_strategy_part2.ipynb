{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RAG chunking strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # inside a script\n",
    "\tBASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "\tBASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: /Users/tejaspancholi/Developer/python/vizuara\n"
     ]
    }
   ],
   "source": [
    "print(f\"Project root set to: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = BASE_DIR / \"data\" / \"human_nutrition_text.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_requests(\n",
    "\turl: str, dest: Path, timeout: int = 30, max_retries: int = 3\n",
    ") -> None:\n",
    "\t\"\"\"Download a PDF file from URL with progress tracking and error handling.\"\"\"\n",
    "\tdest.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tfor attempt in range(max_retries):\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(url, stream=True, timeout=timeout)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\n",
    "\t\t\tcontent_type = response.headers.get(\"content-type\", \"\").lower()\n",
    "\t\t\tif \"pdf\" not in content_type:\n",
    "\t\t\t\traise ValueError(f\"Invalid content type: {content_type}\")\n",
    "\t\t\ttotal = int(response.headers.get(\"content-length\", 0))\n",
    "\t\t\twith tqdm(\n",
    "\t\t\t\ttotal=total, unit=\"iB\", unit_scale=True, desc=\"Downloading PDF\"\n",
    "\t\t\t) as t:\n",
    "\t\t\t\twith dest.open(\"wb\") as f:\n",
    "\t\t\t\t\tfor chunk in response.iter_content(chunk_size=8192):\n",
    "\t\t\t\t\t\tif chunk:\n",
    "\t\t\t\t\t\t\tf.write(chunk)\n",
    "\t\t\t\t\t\t\tt.update(len(chunk))\n",
    "\t\t\tprint(f\"\\nSuccessfully downloaded PDF to {dest}\")\n",
    "\t\t\treturn\n",
    "\t\texcept requests.exceptions.RequestException as e:\n",
    "\t\t\tprint(f\"Download failed: {e}\")\n",
    "\t\t\tif attempt == max_retries - 1:\n",
    "\t\t\t\traise\n",
    "\t\t\ttime.sleep(2**attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pdf_path.is_file():\n",
    "\tdownload_pdf_requests(\n",
    "\t\t\"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\",\n",
    "\t\tpdf_path,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "\t\"\"\"Performs minor text formatting.\"\"\"\n",
    "\timport re\n",
    "\n",
    "\tcleaned_text = re.sub(\n",
    "\t\tr\"\\s+\", \" \", text\n",
    "\t)  # Replace multiple whitespace with single space\n",
    "\tcleaned_text = cleaned_text.strip()\n",
    "\treturn cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def open_and_read_pdf(file_path: Union[str, Path]) -> Union[List[Dict], None]:\n",
    "\t\"\"\"\n",
    "\tOpens a pdf file and reads its content page by page, and collects statistics.\n",
    "\tParameters:\n",
    "\t    file_path (str | Path): The path to the pdf file to be opened and read.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries containing the page number, character count, word count, sentence count, token count, and extracted text for each page.\n",
    "\t\"\"\"\n",
    "\tif not Path(file_path).exists():\n",
    "\t\traise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "\ttry:\n",
    "\t\tdoc = pymupdf.open(file_path)\n",
    "\t\tpages_and_texts = []\n",
    "\t\tfor page_number, page in tqdm(enumerate(doc)):\n",
    "\t\t\ttext = page.get_text()\n",
    "\t\t\tif not text or not text.strip():  # Skip empty pages\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif text and text.strip():\n",
    "\t\t\t\ttext = text_formatter(text)\n",
    "\t\t\t\tsentences = re.split(r\"[.!?]+\", text)  # Simple sentence splitter\n",
    "\t\t\t\tsentence_count = len(\n",
    "\t\t\t\t\t[s for s in sentences if s.strip()]\n",
    "\t\t\t\t)  # Count non-empty sentences\n",
    "\t\t\t\tpages_and_texts.append(\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"page_number\": page_number - 41,\n",
    "\t\t\t\t\t\t\"page_char_count\": len(text),\n",
    "\t\t\t\t\t\t\"page_word_count\": len(text.split()),\n",
    "\t\t\t\t\t\t\"page_sentence_count_raw\": sentence_count,\n",
    "\t\t\t\t\t\t\"page_token_count\": int(len(text) / 4),\n",
    "\t\t\t\t\t\t\"text\": text,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t)\n",
    "\t\treturn pages_and_texts\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error reading PDF file: {e}\")\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [00:00, 1253.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page_number': -41, 'page_char_count': 29, 'page_word_count': 4, 'page_sentence_count_raw': 1, 'page_token_count': 7, 'text': 'Human Nutrition: 2020 Edition'}, {'page_number': -39, 'page_char_count': 308, 'page_word_count': 42, 'page_sentence_count_raw': 1, 'page_token_count': 77, 'text': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pages_and_texts = open_and_read_pdf(file_path=pdf_path)\n",
    "if pages_and_texts:\n",
    "\tprint(pages_and_texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame(pages_and_texts)\n",
    "summary = df.describe()\n",
    "numeric_cols = [c for c, t in summary.schema.items() if t.is_numeric()]\n",
    "summary = summary.with_columns(\n",
    "\t[pl.col(c).round(2) if c in numeric_cols else pl.col(c) for c in summary.columns]\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2732ee0",
   "metadata": {},
   "source": [
    "## Method 1: Fixed size chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500) -> List[str]:\n",
    "\t\"\"\"Splits text into chunks of specified size with overlap.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    chunk_size (int): The size of each chunk in words.\n",
    "\tReturns:\n",
    "\t    List[str]: A list of text chunks.\n",
    "\t\"\"\"\n",
    "\tchunks = []\n",
    "\tcurrent_chunk = \"\"\n",
    "\twords = text.split()\n",
    "\n",
    "\tfor word in words:\n",
    "\t\tif len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "\t\t\tcurrent_chunk += word + \" \"\n",
    "\t\telse:\n",
    "\t\t\tchunks.append(current_chunk.strip())\n",
    "\t\t\tcurrent_chunk = word + \" \"\n",
    "\n",
    "\tif current_chunk:\n",
    "\t\tchunks.append(current_chunk.strip())\n",
    "\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_pdf_pages(pages_and_texts: list, chunk_size: int = 500) -> List[Dict]:\n",
    "\t\"\"\"Chunks the text of each page into smaller segments.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of dictionaries containing page information and text.\n",
    "\t    chunk_size (int): The size of each chunk in words.\n",
    "\tReturns:\n",
    "\t    List[Dict]: A list of dictionaries with chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tchunked_data = []\n",
    "\n",
    "\tfor page in pages_and_texts:\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = chunk_text(text, chunk_size)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tchunked_data.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a9a7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_pages = chunk_pdf_pages(pages_and_texts, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks created: {len(chunked_pages)}\")\n",
    "print(\n",
    "\tf\"25th chunk (page {chunked_pages[24]['page_number']}): {chunked_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ac431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Generate k scattered indices over range n with some jitter.\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {c['chunk_token_count']}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af832d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_chunks(\n",
    "\tpages_and_texts: list, chunk_size: int = 500, k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    chunk_size (int): Size of each text chunk.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\t# Chunk the text from each page\n",
    "\tall_chunks = []\n",
    "\tall_chunks = chunk_pdf_pages(pages_and_texts, chunk_size)\n",
    "\tif not all_chunks:\n",
    "\t\tprint(\"No chunks available to display.\")\n",
    "\t\treturn\n",
    "\tindices = _scattered_indices(len(all_chunks), k)\n",
    "\tprint(\n",
    "\t\tf\"Showing {len(indices)} scattered random chunks out of {len(all_chunks)} total chunks:\\n\"\n",
    "\t)\n",
    "\tfor i, idx in enumerate(indices, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(all_chunks[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pages_and_texts is not None\n",
    "show_random_chunks(pages_and_texts, chunk_size=500, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189352f",
   "metadata": {},
   "source": [
    "### here you might have seen some chunk are smaller than 500 even though we have mentioned as chunk size as 500, its because the processes is happening at page level and it can happen once a couple of chunk are done at the page, rest of text is smaller than 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e744d6",
   "metadata": {},
   "source": [
    "## Method 2: Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93f1cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ec548b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a609cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk_text(\n",
    "\ttext: str, similarity_threshold: float = 0.8, max_tokens: int = 500\n",
    ") -> list:\n",
    "\t\"\"\"Splits text into semantically coherent chunks based on sentence embeddings.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    similarity_threshold (float): Cosine similarity threshold to determine chunk boundaries.\n",
    "\t    max_tokens (int): Maximum number of tokens per chunk.\n",
    "\tReturns:\n",
    "\t    list: A list of semantically coherent text chunks.\n",
    "\t\"\"\"\n",
    "\tsentences = nltk.sent_tokenize(text)\n",
    "\tif not sentences:\n",
    "\t\treturn []\n",
    "\n",
    "\t# ensure embeddings is a numpy array of shape (n_sentences, dim)\n",
    "\tembeddings = semantic_model.encode(sentences, convert_to_numpy=True)\n",
    "\tif not isinstance(embeddings, np.ndarray):\n",
    "\t\tembeddings = np.array(embeddings)\n",
    "\n",
    "\tchunks = []\n",
    "\tcurrent_chunk = [sentences[0]]\n",
    "\tcurrent_indices = [0]\n",
    "\n",
    "\tfor i in range(1, len(sentences)):\n",
    "\t\tcurrent_embedding = np.mean(embeddings[current_indices], axis=0)\n",
    "\t\tnext_embedding = embeddings[i]\n",
    "\t\tsim = float(\n",
    "\t\t\tcosine_similarity(\n",
    "\t\t\t\tcurrent_embedding.reshape(1, -1), next_embedding.reshape(1, -1)\n",
    "\t\t\t)[0, 0]\n",
    "\t\t)\n",
    "\n",
    "\t\tchunk_token_count = len(\" \".join(current_chunk)) // 4\n",
    "\n",
    "\t\tif sim >= similarity_threshold and chunk_token_count < max_tokens:\n",
    "\t\t\tcurrent_chunk.append(sentences[i])\n",
    "\t\t\tcurrent_indices.append(i)\n",
    "\t\telse:\n",
    "\t\t\tchunks.append(\" \".join(current_chunk))\n",
    "\t\t\tcurrent_chunk = [sentences[i]]\n",
    "\t\t\tcurrent_indices = [i]\n",
    "\tif current_chunk:\n",
    "\t\tchunks.append(\" \".join(current_chunk))\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fee55db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk_pdf_pages(\n",
    "\tpages_and_texts: list, similarity_threshold: float = 0.8, max_tokens: int = 500\n",
    ") -> list[dict]:\n",
    "\t\"\"\"Chunks the text of each page into semantically coherent segments.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of dictionaries containing page information and text.\n",
    "\t    similarity_threshold (float): Cosine similarity threshold to determine chunk boundaries.\n",
    "\t    max_tokens (int): Maximum number of tokens per chunk.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with semantically chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tall_chunks = []\n",
    "\n",
    "\tfor page in tqdm(pages_and_texts, desc=\"Semantic chunking pages\"):\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = semantic_chunk_text(text, similarity_threshold, max_tokens)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tall_chunks.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7feba889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tejaspancholi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Semantic chunking pages: 100%|██████████| 1179/1179 [00:28<00:00, 41.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "semantic_chunk_pages = semantic_chunk_pdf_pages(\n",
    "\tpages_and_texts, similarity_threshold=0.75, max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total semantic chunks created: {len(semantic_chunk_pages)}\")\n",
    "print(\n",
    "\tf\"25th semantic chunk (page {semantic_chunk_pages[24]['page_number']}): {semantic_chunk_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0e6b1",
   "metadata": {},
   "source": [
    "### Number of chunks have drastically increased is because there is very limited similiarity meaning between pages or paras, leading to smaller chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Evenely spaced anchors + random jitters + indices scattered across [0,n-1]\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b431f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\tapprox_tokens = c.get(\"chunk_token_count\", len(c[\"chunk_text\"]) / 4)\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {round(approx_tokens, 2)}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_semantic_random_chunks(\n",
    "\tsemantic_chunked_pages: list[dict], k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the semantic chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\tn = len(semantic_chunked_pages)\n",
    "\tif n == 0:\n",
    "\t\tprint(\"No semantic chunks available to display.\")\n",
    "\t\treturn\n",
    "\tidxs = _scattered_indices(n, k)\n",
    "\tprint(\n",
    "\t\tf\"Showing {len(idxs)} scattered random semantic chunks out of {n} total chunks:\\n\"\n",
    "\t)\n",
    "\tfor i, idx in enumerate(idxs, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(semantic_chunked_pages[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert semantic_chunk_pages is not None\n",
    "show_semantic_random_chunks(semantic_chunk_pages, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b2c9c",
   "metadata": {},
   "source": [
    "## Method 3 - Recursive Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626ed10",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "- if chunk is smaller than `max_chunk_size`, its chunked as is\n",
    "- if its larger, it will try to split by `\\n\\n` which is usually between two sections\n",
    "- if that also does not work it will try to split by `\\n` which is usually a para\n",
    "-  if para is still too large, its split by sentence\n",
    "- repeat the process recursively\n",
    "- *note* the list of separator can be changed, by supplying it during method invocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca23d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunk_text(\n",
    "\ttext: str, max_chunk_size: int = 1000, min_chunk_size: int = 100\n",
    ") -> list:\n",
    "\t\"\"\"Recursively chunks text into smaller segments based on size constraints.\n",
    "\tTries splitting by sections, then newlines, then sentences.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    max_chunk_size (int): Maximum size of each chunk in characters.\n",
    "\t    min_chunk_size (int): Minimum size of each chunk in characters.\n",
    "\tReturns:\n",
    "\t    list: A list of text chunks.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef split_chunk(chunk: str) -> list:\n",
    "\t\tif len(chunk) <= max_chunk_size:\n",
    "\t\t\treturn [chunk.strip()]\n",
    "\t\t# first try splitting by sections (double newlines)\n",
    "\t\tsections = chunk.split(\"\\n\\n\")\n",
    "\t\tif len(sections) > 1:\n",
    "\t\t\tresult = []\n",
    "\t\t\tfor section in sections:\n",
    "\t\t\t\tif section.strip():\n",
    "\t\t\t\t\tresult.extend(split_chunk(section.strip()))\n",
    "\t\t\treturn result\n",
    "\t\t# next try splitting by single newlines\n",
    "\t\tsections = chunk.split(\"\\n\")\n",
    "\t\tif len(sections) > 1:\n",
    "\t\t\tresult = []\n",
    "\t\t\tfor section in sections:\n",
    "\t\t\t\tif section.strip():\n",
    "\t\t\t\t\tresult.extend(split_chunk(section.strip()))\n",
    "\t\t\treturn result\n",
    "\t\t# finally split by sentences\n",
    "\t\tsentences = nltk.sent_tokenize(chunk)\n",
    "\t\tchunks, current_chunk, current_size = [], [], 0\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tif current_size + len(sentence) > max_chunk_size:\n",
    "\t\t\t\tif current_chunk:\n",
    "\t\t\t\t\tchunks.append(\" \".join(current_chunk).strip())\n",
    "\t\t\t\tcurrent_chunk, current_size = [sentence], len(sentence)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_chunk.append(sentence)\n",
    "\t\t\t\tcurrent_size += len(sentence)\n",
    "\t\tif current_chunk:\n",
    "\t\t\tchunks.append(\" \".join(current_chunk).strip())\n",
    "\t\treturn chunks\n",
    "\n",
    "\treturn split_chunk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a98884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunk_pdf_pages(\n",
    "\tpages_and_texts: list, max_chunk_size: int = 1000, min_chunk_size: int = 100\n",
    ") -> list[dict]:\n",
    "\t\"\"\"Chunks the text of each page into smaller segments using recursive chunking.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of dictionaries containing page information and text.\n",
    "\t    max_chunk_size (int): Maximum size of each chunk in characters.\n",
    "\t    min_chunk_size (int): Minimum size of each chunk in characters.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with recursively chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tall_chunks = []\n",
    "\n",
    "\tfor page in tqdm(pages_and_texts, desc=\"Recursive chunking pages\"):\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = recursive_chunk_text(text, max_chunk_size, min_chunk_size)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tall_chunks.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c63bc79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recursive chunking pages: 100%|██████████| 1179/1179 [00:00<00:00, 15581.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recursive chunks created: 1949\n",
      "25th recursive chunk (page -17): Preface UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM ‘A‘ohe pau ka ‘ike i ka hālau ho‘okahi Knowledge isn’t taught in all one place This open acc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recursive_chunked_pages = recursive_chunk_pdf_pages(\n",
    "\tpages_and_texts, max_chunk_size=1000, min_chunk_size=100\n",
    ")\n",
    "print(f\"Total recursive chunks created: {len(recursive_chunked_pages)}\")\n",
    "print(\n",
    "\tf\"25th recursive chunk (page {recursive_chunked_pages[24]['page_number']}): {recursive_chunked_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7593ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Generate k scattered indices over range n with some jitter.\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out\n",
    "\n",
    "\n",
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\tapprox_tokens = c.get(\"chunk_token_count\", len(c[\"chunk_text\"]) / 4)\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {round(approx_tokens, 2)}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])\n",
    "\n",
    "\n",
    "def show_recursive_random_chunks(\n",
    "\trecursive_chunked_pages: list[dict], k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the recursive chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\tn = len(recursive_chunked_pages)\n",
    "\tif n == 0:\n",
    "\t\tprint(\"No recursive chunks available to display.\")\n",
    "\t\treturn\n",
    "\tidxs = _scattered_indices(n, k)\n",
    "\tprint(\n",
    "\t\tf\"Showing {len(idxs)} scattered random recursive chunks out of {n} total chunks:\\n\"\n",
    "\t)\n",
    "\tfor i, idx in enumerate(idxs, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(recursive_chunked_pages[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2000aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert recursive_chunked_pages is not None and len(recursive_chunked_pages) > 0\n",
    "show_recursive_random_chunks(recursive_chunked_pages, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22e0ce",
   "metadata": {},
   "source": [
    "## Method 4: Structured based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bfb6c",
   "metadata": {},
   "source": [
    "### How it works\n",
    "- The function looks for `headers` such as Chapter numbers(e.g. CHAPTER 1) or section heading(i.e. 1.1 Introduction)\n",
    "- Every time it finds a header it starts a new chunk till either another heading is reached or max token size limit is exceeded\n",
    "- This preserves logical flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd573c6",
   "metadata": {},
   "source": [
    "### Engineer's decision\n",
    "- Works well with documents which have clear hierarchy(chapter, section, subsection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db999415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to detect chapter start\n",
    "def _is_chapter_start(text: str) -> bool:\n",
    "\t\"\"\"Detects if a line indicates the start of a new chapter or section.\"\"\"\n",
    "\t# chapter_patterns = [\n",
    "\t#     r'^\\s*CHAPTER\\s+\\d+',  # Matches \"CHAPTER 1\", \"CHAPTER 2\", etc.\n",
    "\t#     r'^\\s*\\d+(\\.\\d+)*\\s+[A-Z][a-zA-Z\\s]*',  # Matches \"1. Introduction\", \"2.1 Background\", etc.\n",
    "\t# ]\n",
    "\t# for pattern in chapter_patterns:\n",
    "\t#     if re.match(pattern, line):\n",
    "\t#         return True\n",
    "\t# return False\n",
    "\treturn re.search(r\"university\\s+of\\s+hawai\", text, flags=re.IGNORECASE) is not None\n",
    "\n",
    "\n",
    "def _guess_title_from_page(text: str) -> str:\n",
    "\t\"\"\"the previous line of 'University of Hawaii' is likely the title\n",
    "\tfalls back to the first ~120 characters\n",
    "\tArgs:\n",
    "\t    text (str): The text of the page.\n",
    "\tReturns:\n",
    "\t    str: The guessed title of the page.\n",
    "\t\"\"\"\n",
    "\tmatch = re.search(r\"university\\s+of\\s+hawai\", text, flags=re.IGNORECASE)\n",
    "\tif match:\n",
    "\t\ttitle = text[: match.start()].strip()\n",
    "\t\ttitle = re.sub(r\"\\s+\", \" \", title.strip())  # Clean up whitespace\n",
    "\t\tif 10 <= len(title) <= 180:\n",
    "\t\t\treturn title\n",
    "\t# fallback to first ~120 characters\n",
    "\ttitle = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\treturn title[:120] if title else \"Untitled\"\n",
    "\n",
    "\n",
    "def chapter_chunk_pdf_pages(pages_and_texts: list) -> list[dict]:\n",
    "\t\"\"\"\n",
    "\tChunks PDF pages into sections based on chapter titles.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with chapter titles and associated pages.\n",
    "\t\"\"\"\n",
    "\tif not pages_and_texts:\n",
    "\t\treturn []\n",
    "\tchapter_starts = []\n",
    "\tfor i, page in enumerate(pages_and_texts):\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tif _is_chapter_start(text):\n",
    "\t\t\tchapter_starts.append(i)\n",
    "\t# if nothing detected, return all pages as a single chunk\n",
    "\tif not chapter_starts:\n",
    "\t\t# No chapters found, return all pages as a single chunk\n",
    "\t\tall_text = \" \".join(page[\"text\"] for page in pages_and_texts).strip()\n",
    "\t\treturn [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"chapter_index\": 0,\n",
    "\t\t\t\t\"chapter_title\": _guess_title_from_page(pages_and_texts[0][\"text\"]),\n",
    "\t\t\t\t\"start_page\": pages_and_texts[0][\"page_number\"],\n",
    "\t\t\t\t\"end_page\": pages_and_texts[-1][\"page_number\"],\n",
    "\t\t\t\t\"chunk_char_count\": len(all_text),\n",
    "\t\t\t\t\"chunk_word_count\": len(all_text.split()),\n",
    "\t\t\t\t\"chunk_token_count\": int(len(all_text) / 4),\n",
    "\t\t\t\t\"chunk_text\": all_text,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t# build chapter ranges (start -> next start - 1)\n",
    "\tchapter_chunks = []\n",
    "\tfor idx, start in enumerate(chapter_starts):\n",
    "\t\tend = (\n",
    "\t\t\tchapter_starts[idx + 1] - 1\n",
    "\t\t\tif idx + 1 < len(chapter_starts)\n",
    "\t\t\telse len(pages_and_texts) - 1\n",
    "\t\t)\n",
    "\t\tif end < start:\n",
    "\t\t\tcontinue  # skip invalid ranges\n",
    "\t\tchapter_pages = pages_and_texts[start : end + 1]\n",
    "\t\tchapter_text = \" \".join(page[\"text\"] for page in chapter_pages).strip()\n",
    "\t\tchapter_title = _guess_title_from_page(chapter_pages[0][\"text\"])\n",
    "\t\tchapter_chunks.append(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"chapter_index\": idx,\n",
    "\t\t\t\t\"chapter_title\": chapter_title,\n",
    "\t\t\t\t\"start_page\": chapter_pages[0][\"page_number\"],\n",
    "\t\t\t\t\"end_page\": chapter_pages[-1][\"page_number\"],\n",
    "\t\t\t\t\"chunk_char_count\": len(chapter_text),\n",
    "\t\t\t\t\"chunk_word_count\": len(chapter_text.split()),\n",
    "\t\t\t\t\"chunk_token_count\": int(len(chapter_text) / 4),\n",
    "\t\t\t\t\"chunk_text\": chapter_text,\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\treturn chapter_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fff3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chapter chunks created: 171\n",
      "1st chapter chunk (pages -39 to -39): Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FI...\n"
     ]
    }
   ],
   "source": [
    "structured_chunk_pages = chapter_chunk_pdf_pages(pages_and_texts)\n",
    "print(f\"Total chapter chunks created: {len(structured_chunk_pages)}\")\n",
    "print(\n",
    "\tf\"1st chapter chunk (pages {structured_chunk_pages[0]['start_page']} to {structured_chunk_pages[0]['end_page']}): {structured_chunk_pages[0]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\tapprox_tokens = c.get(\"chunk_token_count\", len(c[\"chunk_text\"]) / 4)\n",
    "\theader = (\n",
    "\t\tf\" Chapter idx {c['chapter_index']} - '{c['chapter_title']}'  | \"\n",
    "\t\tf\"pages {c['start_page']} to {c['end_page']} | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {round(approx_tokens, 2)}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])\n",
    "\n",
    "\n",
    "def show_chapter_random_chunks(\n",
    "\tstructured_chunked_pages: list[dict], k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the chapter chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\tn = len(structured_chunked_pages)\n",
    "\tif n == 0:\n",
    "\t\tprint(\"No chapter chunks available to display.\")\n",
    "\t\treturn\n",
    "\tk = min(k, len(structured_chunked_pages))\n",
    "\tidxs = random.sample(range(n), k)\n",
    "\tprint(f\"Showing {len(idxs)} random chapter chunks out of {n} total chunks:\\n\")\n",
    "\tfor i, idx in enumerate(idxs, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(structured_chunked_pages[idx]))\n",
    "\t\tprint()  # extra newline between chunks\n",
    "\n",
    "\n",
    "assert structured_chunk_pages is not None and len(structured_chunk_pages) > 0\n",
    "show_chapter_random_chunks(structured_chunk_pages, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cbce1b",
   "metadata": {},
   "source": [
    "## Strategy 5: LLM Based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_chunk(\n",
    "\ttext: str, chunk_size: int = 500, model: str = \"gpt-4.1-mini\"\n",
    ") -> list[str]:\n",
    "\t\"\"\"\n",
    "\tUses OpenAI's GPT model to chunk text into smaller segments.\n",
    "\tArgs:\n",
    "\t    text (str): The text to be chunked.\n",
    "\t    chunk_size (int): The desired size of each chunk in words.\n",
    "\t    model (str): The OpenAI model to use for chunking.\n",
    "\tReturns:\n",
    "\t    list[str]: A list of text chunks.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef get_chunk_boundary(text_sequence: str) -> int:\n",
    "\t\t\"\"\"Helper function to get chunk boundary from LLM.\n",
    "\t\tArgs:\n",
    "\t\t    text_sequence (str): The text sequence to analyze.\n",
    "\t\tReturns:\n",
    "\t\t    int: The index to split the text at.\n",
    "\t\t\"\"\"\n",
    "\t\tprompt = f\"\"\"\n",
    "        Analyze the following text and identify the best point to split it into two semantically coherent parts.\n",
    "        The split should occur near the {chunk_size} characters.\n",
    "        Text:\n",
    "        \\\"\\\"\\\"{text_sequence}\\\"\\\"\\\"\n",
    "        Return only the integer index (character position) where the split should occur.  \n",
    "        Do not return explanations or any other text.\n",
    "        \"\"\"\n",
    "\n",
    "\t\tresponse = client.chat.completions.create(\n",
    "\t\t\tmodel=model,\n",
    "\t\t\tmessages=[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"role\": \"system\",\n",
    "\t\t\t\t\t\"content\": \"You are a text analysis expert that identifies text chunk boundaries.\",\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\"role\": \"user\", \"content\": prompt},\n",
    "\t\t\t],\n",
    "\t\t\ttemperature=0.0,\n",
    "\t\t)\n",
    "\t\tif response is None or not response.choices[0].message.content:\n",
    "\t\t\treturn chunk_size  # fallback to default chunk size\n",
    "\t\telse:\n",
    "\t\t\tif response.choices[0].message.content.strip().isdigit() is False:\n",
    "\t\t\t\treturn chunk_size  # fallback to default chunk size\n",
    "\t\t\telse:\n",
    "\t\t\t\tsplit_index_str = (response.choices[0].message.content).strip()\n",
    "\t\t\t\tsplit_index = int(split_index_str)\n",
    "\t\t\t\treturn split_index\n",
    "\n",
    "\tchunks = []\n",
    "\tremaining_text = text\n",
    "\n",
    "\twhile len(remaining_text) > chunk_size:\n",
    "\t\ttext_window = remaining_text[: chunk_size * 2]\n",
    "\t\tsplit_index_str = get_chunk_boundary(text_window)\n",
    "\t\tif split_index_str < 100 or split_index_str > len(text_window) - 100:\n",
    "\t\t\tsplit_index_str = chunk_size\n",
    "\t\tchunks.append(remaining_text[:split_index_str].strip())\n",
    "\t\tremaining_text = remaining_text[split_index_str:].strip()\n",
    "\tif remaining_text:\n",
    "\t\tchunks.append(remaining_text)\n",
    "\treturn chunks\n",
    "\n",
    "\n",
    "def llm_based_chunk_pdf_pages(\n",
    "\tpages_and_texts: list[dict], chunk_size: int = 1000, model: str = \"gpt-4.1-mini\"\n",
    ") -> list[dict]:\n",
    "\t\"\"\"\n",
    "\tChunks the text of each page into smaller segments using LLM-based chunking.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list[dict]): List of dictionaries containing page information and text.\n",
    "\t    chunk_size (int): The desired size of each chunk in words.\n",
    "\t    model (str): The OpenAI model to use for chunking.\n",
    "\tReturns:\n",
    "\t    list[dict]: A list of dictionaries with LLM chunked text and associated metadata.\n",
    "\t\"\"\"\n",
    "\tall_chunks = []\n",
    "\n",
    "\tfor page in tqdm(pages_and_texts, desc=\"LLM chunking pages\"):\n",
    "\t\tpage_number = page[\"page_number\"]\n",
    "\t\ttext = page[\"text\"]\n",
    "\t\tchunks = llm_based_chunk(text, chunk_size, model)\n",
    "\n",
    "\t\tfor i, chunk in enumerate(chunks):\n",
    "\t\t\tall_chunks.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"page_number\": page_number,\n",
    "\t\t\t\t\t\"chunk_index\": i,\n",
    "\t\t\t\t\t\"chunk_text\": chunk,\n",
    "\t\t\t\t\t\"chunk_word_count\": len(chunk.split()),\n",
    "\t\t\t\t\t\"chunk_char_count\": len(chunk),\n",
    "\t\t\t\t\t\"chunk_token_count\": int(len(chunk) / 4),\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\treturn all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chunked_pages = llm_based_chunk_pdf_pages(\n",
    "\tpages_and_texts, chunk_size=500, model=\"gpt-4.1-mini\"\n",
    ")\n",
    "print(f\"Total LLM chunks created: {len(llm_chunked_pages)}\")\n",
    "print(\n",
    "\tf\"25th LLM chunk (page {llm_chunked_pages[24]['page_number']}): {llm_chunked_pages[24]['chunk_text'][:200]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "622c9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def _scattered_indices(n: int, k: int, jitter_frac: float = 0.08) -> list[int]:\n",
    "\t\"\"\"Generate k scattered indices over range n with some jitter.\n",
    "\tArgs:\n",
    "\t    n (int): The total number of items.\n",
    "\t    k (int): The number of indices to generate.\n",
    "\t    jitter_frac (float): Fractional jitter to apply to each index.\n",
    "\tReturns:\n",
    "\t    list[int]: A list of k scattered indices.\n",
    "\t\"\"\"\n",
    "\tif k <= 0:\n",
    "\t\treturn []\n",
    "\tif k == 1:\n",
    "\t\treturn [random.randrange(n)]\n",
    "\tanchors = [int(round(i * (n - 1) / (k - 1))) for i in range(k)]\n",
    "\tout, seen = [], set()\n",
    "\tradius = max(1, int(jitter_frac * n))\n",
    "\tfor a in anchors:\n",
    "\t\tlo, hi = max(0, a - radius), min(n - 1, a + radius)\n",
    "\t\tj = random.randint(lo, hi)\n",
    "\t\tif j not in seen:\n",
    "\t\t\tout.append(j)\n",
    "\t\t\tseen.add(j)\n",
    "\twhile len(out) < k:\n",
    "\t\tr = random.randrange(n)\n",
    "\t\tif r not in seen:\n",
    "\t\t\tout.append(r)\n",
    "\t\t\tseen.add(r)\n",
    "\treturn out\n",
    "\n",
    "\n",
    "def _draw_boxed_chunk(c: dict, wrap_at: int = 96) -> str:\n",
    "\t\"\"\"Draws a boxed representation of a text chunk.\n",
    "\tArgs:\n",
    "\t    c (dict): A dictionary containing chunk metadata and text.\n",
    "\t    wrap_at (int): The maximum width for text wrapping.\n",
    "\tReturns:\n",
    "\t    str: A string representation of the boxed chunk.\n",
    "\t\"\"\"\n",
    "\tapprox_tokens = c.get(\"chunk_token_count\", len(c[\"chunk_text\"]) / 4)\n",
    "\theader = (\n",
    "\t\tf\" Chunk p{c['page_number']} - idx {c['chunk_index']}  | \"\n",
    "\t\tf\"chars {c['chunk_char_count']} - words {c['chunk_word_count']} - tokens {round(approx_tokens, 2)}\"\n",
    "\t)\n",
    "\t# wrap body text, avoid breaking long words awkwardly\n",
    "\twrapped_lines = textwrap.wrap(\n",
    "\t\tc[\"chunk_text\"], width=wrap_at, break_long_words=False, replace_whitespace=False\n",
    "\t)\n",
    "\tcontent_width = max([0, *map(len, wrapped_lines)])\n",
    "\tbox_width = max(len(header), content_width + 2)  # +2 - side padding\n",
    "\n",
    "\ttop = \"┌\" + \"─\" * (box_width) + \"┐\"\n",
    "\thline = \"|\" + header.ljust(box_width) + \"|\"\n",
    "\tsep = \"├\" + \"─\" * (box_width) + \"┤\"\n",
    "\tbody = \"\\n\".join(\n",
    "\t\t\"│ \" + line.ljust(box_width - 2) + \" │\" for line in wrapped_lines\n",
    "\t) or (\"|\" + \"\".ljust(box_width - 2) + \" |\")\n",
    "\tbottom = \"└\" + \"─\" * (box_width) + \"┘\"\n",
    "\treturn \"\\n\".join([top, hline, sep, body, bottom])\n",
    "\n",
    "\n",
    "def show_llm_random_chunks(\n",
    "\tllm_chunked_pages: list[dict], k: int = 5, seed: int | None = 42\n",
    ") -> None:\n",
    "\t\"\"\"Displays n random chunks from the LLM chunked pages.\n",
    "\tArgs:\n",
    "\t    pages_and_texts (list): List of tuples (page_number, text) for each page.\n",
    "\t    k (int): Number of random chunks to display.\n",
    "\t    seed (int | None): Random seed for reproducibility.\n",
    "\t\"\"\"\n",
    "\tif seed is not None:\n",
    "\t\trandom.seed(seed)\n",
    "\n",
    "\tn = len(llm_chunked_pages)\n",
    "\tif n == 0:\n",
    "\t\tprint(\"No LLM chunks available to display.\")\n",
    "\t\treturn\n",
    "\tidxs = _scattered_indices(n, k)\n",
    "\tprint(f\"Showing {len(idxs)} scattered random LLM chunks out of {n} total chunks:\\n\")\n",
    "\tfor i, idx in enumerate(idxs, 1):\n",
    "\t\tprint(f\"#{i}\")\n",
    "\t\tprint(_draw_boxed_chunk(llm_chunked_pages[idx]))\n",
    "\t\tprint()  # extra newline between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42e320ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 5 scattered random LLM chunks out of 3313 total chunks:\n",
      "\n",
      "#1\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p-8 - idx 0  | chars 315 - words 44 - tokens 78                                            |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Gemady Langfelder Gemady Langfelder is an undergraduate dietetics student at the University of   │\n",
      "│ Hawai‘i at Mānoa. She is an ACSM certified personal trainer and a novice horticulturist. Her     │\n",
      "│ interests are nutritional epidemiology, infant and pre-/post-natal nutrition, and health policy. │\n",
      "│ xxxiv | About the Contributors                                                                   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#2\n",
      "┌─────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p198 - idx 0  | chars 497 - words 71 - tokens 124                                         |\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ foods. Fresh and frozen foods are better sources of potassium than canned. Learning Activities  │\n",
      "│ Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER)      │\n",
      "│ textbook features interactive learning activities. These activities are available in the web-   │\n",
      "│ based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or │\n",
      "│ Open Document). Learning activities may be used across various mobile devices, however, for the │\n",
      "│ best user experience it is                                                                      │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#3\n",
      "┌─────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p578 - idx 0  | chars 499 - words 83 - tokens 124                                         |\n",
      "├─────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Food Serving Vitamin B6 (mg) Percent Daily Value Chickpeas 1 c. 1.1 55 Tuna, fresh 3 oz. 0.9 45 │\n",
      "│ Salmon 3 oz. 0.6 30 Potatoes 1 c. 0.4 20 Banana 1 medium 0.4 20 Ground beef patty 3 oz. 0.3 10  │\n",
      "│ White rice, enriched 1 c. 0.1 5 Spinach ½ c 0.1 5 Dietary Supplement Fact Sheet: Vitamin B6.    │\n",
      "│ National Institute of Health, Office of Dietary Supplements. https://ods.od.nih.gov/            │\n",
      "│ factsheets/VitaminB6-HealthProfessional/. Updates February 11, 2016. Accessed October 22, 2017. │\n",
      "│ Folate Folate is a required coenzy                                                              │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#4\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p882 - idx 0  | chars 499 - words 71 - tokens 124                                          |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Treatment for lead poisoning includes removing the child from the source of contamination and    │\n",
      "│ extracting lead from the body. Extraction may involve chelation therapy, which binds with lead   │\n",
      "│ so it can be excreted in urine. Another treatment protocol, EDTA therapy, involves administering │\n",
      "│ a drug called ethylenediaminetetraacetic acid to remove lead from the bloodstream of patients    │\n",
      "│ with levels greater than 45 mcg/dL.9 Fortunately, lead toxicity is highly preventable. It        │\n",
      "│ involves identifying potentia                                                                    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "#5\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "| Chunk p1117 - idx 1  | chars 453 - words 57 - tokens 113                                         |\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ glycerides greater than 150 mg/dL; high density lipoproteins (HDL) lower than 40 mg/dL; systolic │\n",
      "│ blood pressure above 100 mmHg, or diastolic above 85 mmHg; fasting blood-glucose levels greater  │\n",
      "│ than 100 mg/dL.16 The IDF estimates that between 20 and 16. The IDF Consensus Worldwide          │\n",
      "│ Definition of the Metabolic Syndrome. International Diabetes Federation.https://www.idf.org/our- │\n",
      "│ activities/ advocacy-awareness/resources-and-tools/ Threats to Health | 1117                     │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert llm_chunked_pages is not None and len(llm_chunked_pages) > 0\n",
    "show_llm_random_chunks(llm_chunked_pages, k=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac407a4a",
   "metadata": {},
   "source": [
    "## Analysis of chunking methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "390dd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# metric can be \"chars\", \"words\", or \"tokens\"\n",
    "\n",
    "METRIC = \"words\"\n",
    "\n",
    "\n",
    "def _size_val(c, metric: str):\n",
    "\tif metric == \"chars\":\n",
    "\t\treturn c.get(\"chunk_char_count\", len(c.get(\"chunk_text\", \"\")))\n",
    "\telif metric == \"words\":\n",
    "\t\treturn c.get(\"chunk_word_count\", len(c.get(\"chunk_text\", \"\").split()))\n",
    "\telif metric == \"tokens\":\n",
    "\t\treturn c.get(\"chunk_token_count\", len(c.get(\"chunk_text\", \"\")) / 4)\n",
    "\telse:\n",
    "\t\traise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "def analyze_chunks(chunks: list[dict], method_name: str, metric: str = \"words\") -> dict:\n",
    "\t\"\"\"Analyzes chunk sizes and provides statistics and histogram data.\n",
    "\tArgs:\n",
    "\t    chunks (list[dict]): List of chunk dictionaries.\n",
    "\t    method_name (str): The name of the chunking method used.\n",
    "\t    metric (str): The metric to analyze (\"chars\", \"words\", or \"tokens\").\n",
    "\tReturns:\n",
    "\t    dict: A dictionary containing analysis results.\n",
    "\t\"\"\"\n",
    "\tif not chunks:\n",
    "\t\treturn {}\n",
    "\n",
    "\tsizes = [_size_val(c, metric) for c in chunks]\n",
    "\tsizes = [s for s in sizes if s > 0]  # filter out non-positive sizes\n",
    "\tif not sizes:\n",
    "\t\treturn {}\n",
    "\n",
    "\tsizes_array = np.array(sizes)\n",
    "\tanalysis = {\n",
    "\t\t\"method\": method_name,\n",
    "\t\t\"# of chunks\": len(sizes),\n",
    "\t\t\"min chunk size\": int(np.min(sizes_array)),\n",
    "\t\t\"max chunk size\": int(np.max(sizes_array)),\n",
    "\t\t\"avg. chunk size\": float(np.mean(sizes_array)),\n",
    "\t\t\"median chunk size\": float(np.median(sizes_array)),\n",
    "\t}\n",
    "\n",
    "\treturn analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7879d257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           method  # of chunks  min chunk size  max chunk size  avg. chunk size  median chunk size\n",
      "   Chapter Chunks          171              25            8900         1214.801              819.0\n",
      " Recursive Chunks         1949               3             227          106.586              122.0\n",
      " LLM-based Chunks         3313               1             117           63.141               74.0\n",
      "Fixed Size Chunks         3321               1             117           62.552               73.0\n",
      "  Semantic Chunks        12027               1             227           17.273               14.0\n",
      "\n",
      " Performance Analysis\n",
      "1. Fixed Size Chunks: Simple and fast, but may split sentences awkwardly.\n",
      "2. Semantic Chunks: More coherent chunks, but requires embedding computation.\n",
      "3. Recursive Chunks: Balances size and coherence, good for varied text.\n",
      "4. Structure based Chunks: Best for structured documents, but relies on clear headings.\n",
      "5. LLM-based Chunks: Highly coherent, but slower and dependent on API calls.\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "\t(\"Fixed Size Chunks\", chunked_pages),\n",
    "\t(\"Semantic Chunks\", semantic_chunk_pages),\n",
    "\t(\"Recursive Chunks\", recursive_chunked_pages),\n",
    "\t(\"Chapter Chunks\", structured_chunk_pages),\n",
    "\t(\"LLM-based Chunks\", llm_chunked_pages),\n",
    "]\n",
    "\n",
    "results = [analyze_chunks(chunks, name, METRIC) for name, chunks in datasets]\n",
    "df = pd.DataFrame(results).dropna().sort_values(by=\"avg. chunk size\", ascending=False)\n",
    "print(df.round(3).to_string(index=False))\n",
    "\n",
    "print(\"\\n Performance Analysis\")\n",
    "print(\"1. Fixed Size Chunks: Simple and fast, but may split sentences awkwardly.\")\n",
    "print(\"2. Semantic Chunks: More coherent chunks, but requires embedding computation.\")\n",
    "print(\"3. Recursive Chunks: Balances size and coherence, good for varied text.\")\n",
    "print(\n",
    "\t\"4. Structure based Chunks: Best for structured documents, but relies on clear headings.\"\n",
    ")\n",
    "print(\"5. LLM-based Chunks: Highly coherent, but slower and dependent on API calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08833220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizuara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
